[INFO][2021-03-07 01:28:47 357][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]-[The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.]
[INFO][2021-03-07 01:28:47 358][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]-[The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.]
[INFO][2021-03-07 01:28:47 358][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]-[The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.]
[INFO][2021-03-07 01:28:47 358][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]-[The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.]
[INFO][2021-03-07 01:28:47 358][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]-[The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.]
[INFO][2021-03-07 01:28:47 358][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]-[The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.]
[INFO][2021-03-07 01:28:47 370][org.apache.flink.runtime.minicluster.MiniCluster]-[Starting Flink Mini Cluster]
[INFO][2021-03-07 01:28:47 374][org.apache.flink.runtime.minicluster.MiniCluster]-[Starting Metrics Registry]
[INFO][2021-03-07 01:28:47 433][org.apache.flink.runtime.metrics.MetricRegistryImpl]-[No metrics reporter configured, no metrics will be exposed/reported.]
[INFO][2021-03-07 01:28:47 433][org.apache.flink.runtime.minicluster.MiniCluster]-[Starting RPC Service(s)]
[INFO][2021-03-07 01:28:47 440][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]-[Trying to start local actor system]
[INFO][2021-03-07 01:28:47 767][akka.event.slf4j.Slf4jLogger]-[Slf4jLogger started]
[INFO][2021-03-07 01:28:48 020][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]-[Actor system started at akka://flink]
[INFO][2021-03-07 01:28:48 047][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]-[Trying to start local actor system]
[INFO][2021-03-07 01:28:48 077][akka.event.slf4j.Slf4jLogger]-[Slf4jLogger started]
[INFO][2021-03-07 01:28:48 127][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]-[Actor system started at akka://flink-metrics]
[INFO][2021-03-07 01:28:48 144][org.apache.flink.runtime.rpc.akka.AkkaRpcService]-[Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .]
[INFO][2021-03-07 01:28:48 217][org.apache.flink.runtime.minicluster.MiniCluster]-[Starting high-availability services]
[INFO][2021-03-07 01:28:48 231][org.apache.flink.runtime.blob.BlobServer]-[Created BLOB server storage directory /var/folders/r3/pbs1kc5s4b53dx4mw86904gw0000gn/T/blobStore-594db597-d178-4be1-8d53-6d6983d2bf06]
[INFO][2021-03-07 01:28:48 239][org.apache.flink.runtime.blob.BlobServer]-[Started BLOB server at 0.0.0.0:49633 - max concurrent requests: 50 - max backlog: 1000]
[INFO][2021-03-07 01:28:48 243][org.apache.flink.runtime.blob.PermanentBlobCache]-[Created BLOB cache storage directory /var/folders/r3/pbs1kc5s4b53dx4mw86904gw0000gn/T/blobStore-c88b383a-541f-432f-a4c9-dc5693b716a3]
[INFO][2021-03-07 01:28:48 245][org.apache.flink.runtime.blob.TransientBlobCache]-[Created BLOB cache storage directory /var/folders/r3/pbs1kc5s4b53dx4mw86904gw0000gn/T/blobStore-26537ad1-741f-491e-ac20-45c6a25dcde3]
[INFO][2021-03-07 01:28:48 245][org.apache.flink.runtime.minicluster.MiniCluster]-[Starting 1 TaskManger(s)]
[INFO][2021-03-07 01:28:48 249][org.apache.flink.runtime.taskexecutor.TaskManagerRunner]-[Starting TaskManager with ResourceID: 1a228b00-0115-4502-8d76-6df23e70baeb]
[INFO][2021-03-07 01:28:48 271][org.apache.flink.runtime.taskexecutor.TaskManagerServices]-[Temporary file directory '/var/folders/r3/pbs1kc5s4b53dx4mw86904gw0000gn/T': total 465 GB, usable 215 GB (46.24% usable)]
[INFO][2021-03-07 01:28:48 275][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]-[FileChannelManager uses directory /var/folders/r3/pbs1kc5s4b53dx4mw86904gw0000gn/T/flink-io-3051d246-88a7-4655-89d0-d5f50db48be0 for spill files.]
[INFO][2021-03-07 01:28:48 284][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]-[FileChannelManager uses directory /var/folders/r3/pbs1kc5s4b53dx4mw86904gw0000gn/T/flink-netty-shuffle-b7eaa46c-63c0-423f-8b58-9b5fb9b82f45 for spill files.]
[INFO][2021-03-07 01:28:48 322][org.apache.flink.runtime.io.network.buffer.NetworkBufferPool]-[Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).]
[INFO][2021-03-07 01:28:48 334][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]-[Starting the network environment and its components.]
[INFO][2021-03-07 01:28:48 335][org.apache.flink.runtime.taskexecutor.KvStateService]-[Starting the kvState service and its components.]
[INFO][2021-03-07 01:28:48 365][org.apache.flink.runtime.rpc.akka.AkkaRpcService]-[Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .]
[INFO][2021-03-07 01:28:48 379][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]-[Start job leader service.]
[INFO][2021-03-07 01:28:48 380][org.apache.flink.runtime.filecache.FileCache]-[User file cache uses directory /var/folders/r3/pbs1kc5s4b53dx4mw86904gw0000gn/T/flink-dist-cache-84e919d8-414c-4bd5-a51a-aa85ef81dd28]
[INFO][2021-03-07 01:28:48 439][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]-[Starting rest endpoint.]
[WARN][2021-03-07 01:28:48 532][org.apache.flink.runtime.webmonitor.WebMonitorUtils]-[Log file environment variable 'log.file' is not set.]
[WARN][2021-03-07 01:28:48 532][org.apache.flink.runtime.webmonitor.WebMonitorUtils]-[JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.]
[INFO][2021-03-07 01:28:48 724][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]-[Rest endpoint listening at localhost:49639]
[INFO][2021-03-07 01:28:48 725][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]-[Proposing leadership to contender http://localhost:49639]
[INFO][2021-03-07 01:28:48 728][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]-[http://localhost:49639 was granted leadership with leaderSessionID=17f5946b-c584-455a-aee8-b8eb1e069365]
[INFO][2021-03-07 01:28:48 728][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]-[Received confirmation of leadership for leader http://localhost:49639 , session=17f5946b-c584-455a-aee8-b8eb1e069365]
[INFO][2021-03-07 01:28:48 745][org.apache.flink.runtime.rpc.akka.AkkaRpcService]-[Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .]
[INFO][2021-03-07 01:28:48 759][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]-[Proposing leadership to contender LeaderContender: DefaultDispatcherRunner]
[INFO][2021-03-07 01:28:48 760][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]-[Proposing leadership to contender LeaderContender: StandaloneResourceManager]
[INFO][2021-03-07 01:28:48 762][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]-[ResourceManager akka://flink/user/rpc/resourcemanager_1 was granted leadership with fencing token 93e175dd1e667a5b145698644e734e90]
[INFO][2021-03-07 01:28:48 764][org.apache.flink.runtime.minicluster.MiniCluster]-[Flink Mini Cluster started successfully]
[INFO][2021-03-07 01:28:48 766][org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl]-[Starting the SlotManager.]
[INFO][2021-03-07 01:28:48 766][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]-[Start SessionDispatcherLeaderProcess.]
[INFO][2021-03-07 01:28:48 768][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]-[Recover all persisted job graphs.]
[INFO][2021-03-07 01:28:48 768][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]-[Successfully recovered 0 persisted job graphs.]
[INFO][2021-03-07 01:28:48 770][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]-[Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=14569864-4e73-4e90-93e1-75dd1e667a5b]
[INFO][2021-03-07 01:28:48 772][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(93e175dd1e667a5b145698644e734e90).]
[INFO][2021-03-07 01:28:48 777][org.apache.flink.runtime.rpc.akka.AkkaRpcService]-[Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .]
[INFO][2021-03-07 01:28:48 786][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]-[Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=c0245a73-c746-4059-b545-1f76bea8d9f7]
[INFO][2021-03-07 01:28:48 854][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Resolved ResourceManager address, beginning registration]
[INFO][2021-03-07 01:28:48 953][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]-[Registering TaskManager with ResourceID 1a228b00-0115-4502-8d76-6df23e70baeb (akka://flink/user/rpc/taskmanager_0) at ResourceManager]
[INFO][2021-03-07 01:28:48 985][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id 4b46046ed8fda3387dee07cbf0c43a55.]
[INFO][2021-03-07 01:28:49 022][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]-[Received JobGraph submission cf4e2d9502fa4cbfde3e2b72ac2f75de (Flink Streaming Job).]
[INFO][2021-03-07 01:28:49 036][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]-[Submitting job cf4e2d9502fa4cbfde3e2b72ac2f75de (Flink Streaming Job).]
[INFO][2021-03-07 01:28:49 165][org.apache.flink.runtime.rpc.akka.AkkaRpcService]-[Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .]
[INFO][2021-03-07 01:28:49 179][org.apache.flink.runtime.jobmaster.JobMaster]-[Initializing job Flink Streaming Job (cf4e2d9502fa4cbfde3e2b72ac2f75de).]
[INFO][2021-03-07 01:28:49 207][org.apache.flink.runtime.jobmaster.JobMaster]-[Using restart back off time strategy NoRestartBackoffTimeStrategy for Flink Streaming Job (cf4e2d9502fa4cbfde3e2b72ac2f75de).]
[INFO][2021-03-07 01:28:49 250][org.apache.flink.runtime.jobmaster.JobMaster]-[Running initialization on master for job Flink Streaming Job (cf4e2d9502fa4cbfde3e2b72ac2f75de).]
[INFO][2021-03-07 01:28:49 250][org.apache.flink.runtime.jobmaster.JobMaster]-[Successfully ran initialization on master in 0 ms.]
[INFO][2021-03-07 01:28:49 268][org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology]-[Built 1 pipelined regions in 0 ms]
[INFO][2021-03-07 01:28:49 282][org.apache.flink.runtime.jobmaster.JobMaster]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 294][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]-[No checkpoint found during restore.]
[INFO][2021-03-07 01:28:49 296][org.apache.flink.runtime.jobmaster.JobMaster]-[Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@5491e88e for Flink Streaming Job (cf4e2d9502fa4cbfde3e2b72ac2f75de).]
[INFO][2021-03-07 01:28:49 323][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]-[Proposing leadership to contender akka://flink/user/rpc/jobmanager_3]
[INFO][2021-03-07 01:28:49 324][org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl]-[JobManager runner for job Flink Streaming Job (cf4e2d9502fa4cbfde3e2b72ac2f75de) was granted leadership with session id 4037b811-462f-4248-8143-5694341bc951 at akka://flink/user/rpc/jobmanager_3.]
[INFO][2021-03-07 01:28:49 327][org.apache.flink.runtime.jobmaster.JobMaster]-[Starting execution of job Flink Streaming Job (cf4e2d9502fa4cbfde3e2b72ac2f75de) under job master id 81435694341bc9514037b811462f4248.]
[INFO][2021-03-07 01:28:49 329][org.apache.flink.runtime.jobmaster.JobMaster]-[Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]]
[INFO][2021-03-07 01:28:49 329][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Job Flink Streaming Job (cf4e2d9502fa4cbfde3e2b72ac2f75de) switched from state CREATED to RUNNING.]
[INFO][2021-03-07 01:28:49 333][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: Socket Stream (1/1) (73b455378825552a61c8427db7ab7eea) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 334][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, user_info]], fields=[id, name]) (1/1) (85e804a9121b4dc55cd3dd8ad7c56dd3) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 334][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (1/12) (45ff3c022c3bd33d2c80ee20d5052231) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 334][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (2/12) (bef978f6d83df54cc3c6ac6c1cdaca5a) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 334][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (3/12) (fc1e0100f1ada3986192b689441dbbdf) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 334][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (4/12) (ac375608c706165d2a3af65c6877c6cc) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 334][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (5/12) (c3c4c25f49779d48a8c790a1681bf92e) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 334][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (6/12) (1f242ff02c00f8a6380431415e2796d4) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 334][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (7/12) (f725beb86a62a0bd926948e592b4495a) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 334][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (8/12) (2b9dce7547f49f7a4fe30fd1488ecc63) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 334][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (9/12) (bfa16f9c3bcde04c228561ea67f59a25) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 334][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (10/12) (5d646f8fed361a4ebc98c81799f75c29) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 334][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (11/12) (c36f74113749b8ab945e42d2a3412453) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 334][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (12/12) (bd57d9f1a749653d29aca9f59ba8b220) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 334][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (1/12) (d68a4b0e748519771063b5d971bf482f) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 334][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (2/12) (99543f7c156cd9852dde059f337dd602) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 334][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (3/12) (a667de01414f7db1404cb6cb0ee37a05) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 335][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (4/12) (b4d8efc5f3f22e08b27c6219636b9fce) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 335][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (5/12) (ff48db4a5db1743dc62c918a8fd13ea5) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 335][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (6/12) (63e2da992a8707cc702003c12ae8fc47) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 335][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (7/12) (9928e0be325db8629c9d508cd2b437e7) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 335][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (8/12) (14a4e27517cf1a7b8e7cc2c5be6b4175) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 335][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (9/12) (519976251dfcda4d4f597b5c575d4121) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 335][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (10/12) (f807ff0c3cf1d8e4b24931ac9d689322) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 335][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (11/12) (5711d7bb4415778dccdf25165eb69bb2) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 335][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (12/12) (cc1d2fc55dfdb578697413caeb92575d) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 335][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (1/12) (117d90b31e9303984af64aeb0ee12b7b) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 335][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (2/12) (46eb3e3a2642f2cef32223cd9639bafb) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 335][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (3/12) (8313e5af712d2045593482a0820cc361) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 335][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (4/12) (434250ba6208041fd62e2a353b104611) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 335][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (5/12) (73c20f4a24318650c5d7f912965d81cd) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 335][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (6/12) (9c7f4cc7e9397dca962563d379b6807b) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 335][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (7/12) (f8b2448dfa727dda6a93c0ad0d703d2b) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 335][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (8/12) (9969099a68f795d11bfa385055caa659) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 335][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (9/12) (d0b9957e0c5be6e650276ff20a50006e) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 335][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (10/12) (e191529b5423345b14e740a793696c8c) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 335][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (11/12) (9db6958e7070d1ddd8dc0ff744ee978b) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 336][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (12/12) (3be3b9e9654c94309f3c9da98452d53d) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 336][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (1/12) (3a2ad6257fb3a80bcb405ae55a722bf9) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 336][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (2/12) (c04c3ad746a573401a119e7890251457) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 336][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (3/12) (49993121b2bae42027be4a7a79e9f137) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 336][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (4/12) (b99fac6c26d8d31cd214b70ede5f8408) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 336][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (5/12) (a972e3542a745aaface3f6884d3be146) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 336][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (6/12) (303003e33bea497b92f3c6b4ed1fa944) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 336][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (7/12) (979d770095d912b2cf79c375665d892b) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 336][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (8/12) (f8b66729777ee0b7da16ae8c8bca4842) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 336][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (9/12) (3adb7e604c13ae04b4bfe0b9a929e56f) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 336][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (10/12) (2e7d0f61fa31884d84a63807a51288fd) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 336][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (11/12) (b8a21e1f20db61e56e6d176422664e9f) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 336][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (12/12) (32f2fb6525d5156158194b6f80112bb4) switched from CREATED to SCHEDULED.]
[INFO][2021-03-07 01:28:49 350][org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl]-[Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{2b2567dcaf057f7a5148e8967cc875cb}]]
[INFO][2021-03-07 01:28:49 355][org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl]-[Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{d9c64491cd55affa5daedae7cd2fdcb5}]]
[INFO][2021-03-07 01:28:49 355][org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl]-[Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{1c462d44b4b94c315e2eac8e70be2bf1}]]
[INFO][2021-03-07 01:28:49 355][org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl]-[Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{9e8b0abafad47ca2ddbcf82919641120}]]
[INFO][2021-03-07 01:28:49 356][org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl]-[Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{d8d9ba1930a46f8b48f30de35de67ec6}]]
[INFO][2021-03-07 01:28:49 356][org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl]-[Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{764b17fe177c940a8ef970880c994065}]]
[INFO][2021-03-07 01:28:49 356][org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl]-[Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{c95a8a1d4806b361fd736596a6bbf443}]]
[INFO][2021-03-07 01:28:49 356][org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl]-[Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{a8e186d3dd9ef18ab0f78fa869988be9}]]
[INFO][2021-03-07 01:28:49 357][org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl]-[Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{e4615da79594c5662aa854914d5b2173}]]
[INFO][2021-03-07 01:28:49 357][org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl]-[Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{b83f3d11829785354e8be2c815941b26}]]
[INFO][2021-03-07 01:28:49 357][org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl]-[Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{93a7d97946621b4be3f1da0b01345144}]]
[INFO][2021-03-07 01:28:49 358][org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl]-[Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{0673597ee8bbb70178a9db3eec3f2a1c}]]
[INFO][2021-03-07 01:28:49 362][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]-[Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=4037b811-462f-4248-8143-5694341bc951]
[INFO][2021-03-07 01:28:49 362][org.apache.flink.runtime.jobmaster.JobMaster]-[Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(93e175dd1e667a5b145698644e734e90)]
[INFO][2021-03-07 01:28:49 363][org.apache.flink.runtime.jobmaster.JobMaster]-[Resolved ResourceManager address, beginning registration]
[INFO][2021-03-07 01:28:49 365][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]-[Registering job manager 81435694341bc9514037b811462f4248@akka://flink/user/rpc/jobmanager_3 for job cf4e2d9502fa4cbfde3e2b72ac2f75de.]
[INFO][2021-03-07 01:28:49 368][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]-[Registered job manager 81435694341bc9514037b811462f4248@akka://flink/user/rpc/jobmanager_3 for job cf4e2d9502fa4cbfde3e2b72ac2f75de.]
[INFO][2021-03-07 01:28:49 369][org.apache.flink.runtime.jobmaster.JobMaster]-[JobManager successfully registered at ResourceManager, leader id: 93e175dd1e667a5b145698644e734e90.]
[INFO][2021-03-07 01:28:49 370][org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl]-[Requesting new slot [SlotRequestId{2b2567dcaf057f7a5148e8967cc875cb}] and profile ResourceProfile{UNKNOWN} with allocation id 25496b8033d98de9be27839608eee066 from resource manager.]
[INFO][2021-03-07 01:28:49 371][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]-[Request slot with profile ResourceProfile{UNKNOWN} for job cf4e2d9502fa4cbfde3e2b72ac2f75de with allocation id 25496b8033d98de9be27839608eee066.]
[INFO][2021-03-07 01:28:49 371][org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl]-[Requesting new slot [SlotRequestId{d9c64491cd55affa5daedae7cd2fdcb5}] and profile ResourceProfile{UNKNOWN} with allocation id 385c2b39c682065ee1b01a0d53a04242 from resource manager.]
[INFO][2021-03-07 01:28:49 371][org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl]-[Requesting new slot [SlotRequestId{1c462d44b4b94c315e2eac8e70be2bf1}] and profile ResourceProfile{UNKNOWN} with allocation id 3e792c09e4a4b1f17340028d3f492c0b from resource manager.]
[INFO][2021-03-07 01:28:49 371][org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl]-[Requesting new slot [SlotRequestId{9e8b0abafad47ca2ddbcf82919641120}] and profile ResourceProfile{UNKNOWN} with allocation id b16595f537326ab9da08466aa47563d3 from resource manager.]
[INFO][2021-03-07 01:28:49 371][org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl]-[Requesting new slot [SlotRequestId{d8d9ba1930a46f8b48f30de35de67ec6}] and profile ResourceProfile{UNKNOWN} with allocation id eacb317cb275b479e8fcbbb3ea7016a6 from resource manager.]
[INFO][2021-03-07 01:28:49 372][org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl]-[Requesting new slot [SlotRequestId{764b17fe177c940a8ef970880c994065}] and profile ResourceProfile{UNKNOWN} with allocation id 76b0a8587b580146382d96f4d2db67cd from resource manager.]
[INFO][2021-03-07 01:28:49 372][org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl]-[Requesting new slot [SlotRequestId{c95a8a1d4806b361fd736596a6bbf443}] and profile ResourceProfile{UNKNOWN} with allocation id f8689d61dda41c6ded47c656789b3051 from resource manager.]
[INFO][2021-03-07 01:28:49 372][org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl]-[Requesting new slot [SlotRequestId{a8e186d3dd9ef18ab0f78fa869988be9}] and profile ResourceProfile{UNKNOWN} with allocation id a245cfd2a749df92a15769eca12db2be from resource manager.]
[INFO][2021-03-07 01:28:49 372][org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl]-[Requesting new slot [SlotRequestId{e4615da79594c5662aa854914d5b2173}] and profile ResourceProfile{UNKNOWN} with allocation id 3a91bdb43645961c87c8fd500ff54728 from resource manager.]
[INFO][2021-03-07 01:28:49 372][org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl]-[Requesting new slot [SlotRequestId{b83f3d11829785354e8be2c815941b26}] and profile ResourceProfile{UNKNOWN} with allocation id ce26798dd40fb2d0655b066bd1a24372 from resource manager.]
[INFO][2021-03-07 01:28:49 373][org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl]-[Requesting new slot [SlotRequestId{93a7d97946621b4be3f1da0b01345144}] and profile ResourceProfile{UNKNOWN} with allocation id 09e3a764b09fbbbc3e7b5d94131ca3cd from resource manager.]
[INFO][2021-03-07 01:28:49 373][org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl]-[Requesting new slot [SlotRequestId{0673597ee8bbb70178a9db3eec3f2a1c}] and profile ResourceProfile{UNKNOWN} with allocation id 7b41d4e42a57905538a862dc6f41377f from resource manager.]
[INFO][2021-03-07 01:28:49 374][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Receive slot request 25496b8033d98de9be27839608eee066 for job cf4e2d9502fa4cbfde3e2b72ac2f75de from resource manager with leader id 93e175dd1e667a5b145698644e734e90.]
[INFO][2021-03-07 01:28:49 375][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]-[Request slot with profile ResourceProfile{UNKNOWN} for job cf4e2d9502fa4cbfde3e2b72ac2f75de with allocation id 385c2b39c682065ee1b01a0d53a04242.]
[INFO][2021-03-07 01:28:49 376][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]-[Request slot with profile ResourceProfile{UNKNOWN} for job cf4e2d9502fa4cbfde3e2b72ac2f75de with allocation id 3e792c09e4a4b1f17340028d3f492c0b.]
[INFO][2021-03-07 01:28:49 376][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]-[Request slot with profile ResourceProfile{UNKNOWN} for job cf4e2d9502fa4cbfde3e2b72ac2f75de with allocation id b16595f537326ab9da08466aa47563d3.]
[INFO][2021-03-07 01:28:49 376][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]-[Request slot with profile ResourceProfile{UNKNOWN} for job cf4e2d9502fa4cbfde3e2b72ac2f75de with allocation id eacb317cb275b479e8fcbbb3ea7016a6.]
[INFO][2021-03-07 01:28:49 377][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]-[Request slot with profile ResourceProfile{UNKNOWN} for job cf4e2d9502fa4cbfde3e2b72ac2f75de with allocation id 76b0a8587b580146382d96f4d2db67cd.]
[INFO][2021-03-07 01:28:49 377][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]-[Request slot with profile ResourceProfile{UNKNOWN} for job cf4e2d9502fa4cbfde3e2b72ac2f75de with allocation id f8689d61dda41c6ded47c656789b3051.]
[INFO][2021-03-07 01:28:49 377][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]-[Request slot with profile ResourceProfile{UNKNOWN} for job cf4e2d9502fa4cbfde3e2b72ac2f75de with allocation id a245cfd2a749df92a15769eca12db2be.]
[INFO][2021-03-07 01:28:49 377][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]-[Request slot with profile ResourceProfile{UNKNOWN} for job cf4e2d9502fa4cbfde3e2b72ac2f75de with allocation id 3a91bdb43645961c87c8fd500ff54728.]
[INFO][2021-03-07 01:28:49 378][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]-[Request slot with profile ResourceProfile{UNKNOWN} for job cf4e2d9502fa4cbfde3e2b72ac2f75de with allocation id ce26798dd40fb2d0655b066bd1a24372.]
[INFO][2021-03-07 01:28:49 378][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]-[Request slot with profile ResourceProfile{UNKNOWN} for job cf4e2d9502fa4cbfde3e2b72ac2f75de with allocation id 09e3a764b09fbbbc3e7b5d94131ca3cd.]
[INFO][2021-03-07 01:28:49 378][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]-[Request slot with profile ResourceProfile{UNKNOWN} for job cf4e2d9502fa4cbfde3e2b72ac2f75de with allocation id 7b41d4e42a57905538a862dc6f41377f.]
[INFO][2021-03-07 01:28:49 380][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Allocated slot for 25496b8033d98de9be27839608eee066.]
[INFO][2021-03-07 01:28:49 381][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]-[Add job cf4e2d9502fa4cbfde3e2b72ac2f75de for job leader monitoring.]
[INFO][2021-03-07 01:28:49 383][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]-[Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id 4037b811-462f-4248-8143-5694341bc951.]
[INFO][2021-03-07 01:28:49 384][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Receive slot request 385c2b39c682065ee1b01a0d53a04242 for job cf4e2d9502fa4cbfde3e2b72ac2f75de from resource manager with leader id 93e175dd1e667a5b145698644e734e90.]
[INFO][2021-03-07 01:28:49 384][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]-[Resolved JobManager address, beginning registration]
[INFO][2021-03-07 01:28:49 384][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Allocated slot for 385c2b39c682065ee1b01a0d53a04242.]
[INFO][2021-03-07 01:28:49 384][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Receive slot request 3e792c09e4a4b1f17340028d3f492c0b for job cf4e2d9502fa4cbfde3e2b72ac2f75de from resource manager with leader id 93e175dd1e667a5b145698644e734e90.]
[INFO][2021-03-07 01:28:49 385][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Allocated slot for 3e792c09e4a4b1f17340028d3f492c0b.]
[INFO][2021-03-07 01:28:49 385][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Receive slot request b16595f537326ab9da08466aa47563d3 for job cf4e2d9502fa4cbfde3e2b72ac2f75de from resource manager with leader id 93e175dd1e667a5b145698644e734e90.]
[INFO][2021-03-07 01:28:49 385][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Allocated slot for b16595f537326ab9da08466aa47563d3.]
[INFO][2021-03-07 01:28:49 385][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Receive slot request eacb317cb275b479e8fcbbb3ea7016a6 for job cf4e2d9502fa4cbfde3e2b72ac2f75de from resource manager with leader id 93e175dd1e667a5b145698644e734e90.]
[INFO][2021-03-07 01:28:49 385][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Allocated slot for eacb317cb275b479e8fcbbb3ea7016a6.]
[INFO][2021-03-07 01:28:49 385][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Receive slot request 76b0a8587b580146382d96f4d2db67cd for job cf4e2d9502fa4cbfde3e2b72ac2f75de from resource manager with leader id 93e175dd1e667a5b145698644e734e90.]
[INFO][2021-03-07 01:28:49 386][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Allocated slot for 76b0a8587b580146382d96f4d2db67cd.]
[INFO][2021-03-07 01:28:49 386][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Receive slot request f8689d61dda41c6ded47c656789b3051 for job cf4e2d9502fa4cbfde3e2b72ac2f75de from resource manager with leader id 93e175dd1e667a5b145698644e734e90.]
[INFO][2021-03-07 01:28:49 386][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Allocated slot for f8689d61dda41c6ded47c656789b3051.]
[INFO][2021-03-07 01:28:49 386][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Receive slot request a245cfd2a749df92a15769eca12db2be for job cf4e2d9502fa4cbfde3e2b72ac2f75de from resource manager with leader id 93e175dd1e667a5b145698644e734e90.]
[INFO][2021-03-07 01:28:49 386][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Allocated slot for a245cfd2a749df92a15769eca12db2be.]
[INFO][2021-03-07 01:28:49 386][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Receive slot request 3a91bdb43645961c87c8fd500ff54728 for job cf4e2d9502fa4cbfde3e2b72ac2f75de from resource manager with leader id 93e175dd1e667a5b145698644e734e90.]
[INFO][2021-03-07 01:28:49 386][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Allocated slot for 3a91bdb43645961c87c8fd500ff54728.]
[INFO][2021-03-07 01:28:49 386][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Receive slot request ce26798dd40fb2d0655b066bd1a24372 for job cf4e2d9502fa4cbfde3e2b72ac2f75de from resource manager with leader id 93e175dd1e667a5b145698644e734e90.]
[INFO][2021-03-07 01:28:49 387][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Allocated slot for ce26798dd40fb2d0655b066bd1a24372.]
[INFO][2021-03-07 01:28:49 387][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Receive slot request 09e3a764b09fbbbc3e7b5d94131ca3cd for job cf4e2d9502fa4cbfde3e2b72ac2f75de from resource manager with leader id 93e175dd1e667a5b145698644e734e90.]
[INFO][2021-03-07 01:28:49 387][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Allocated slot for 09e3a764b09fbbbc3e7b5d94131ca3cd.]
[INFO][2021-03-07 01:28:49 387][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Receive slot request 7b41d4e42a57905538a862dc6f41377f for job cf4e2d9502fa4cbfde3e2b72ac2f75de from resource manager with leader id 93e175dd1e667a5b145698644e734e90.]
[INFO][2021-03-07 01:28:49 387][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Allocated slot for 7b41d4e42a57905538a862dc6f41377f.]
[INFO][2021-03-07 01:28:49 388][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]-[Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job cf4e2d9502fa4cbfde3e2b72ac2f75de.]
[INFO][2021-03-07 01:28:49 389][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Establish JobManager connection for job cf4e2d9502fa4cbfde3e2b72ac2f75de.]
[INFO][2021-03-07 01:28:49 393][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Offer reserved slots to the leader of job cf4e2d9502fa4cbfde3e2b72ac2f75de.]
[INFO][2021-03-07 01:28:49 403][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: Socket Stream (1/1) (73b455378825552a61c8427db7ab7eea) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 407][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Source: Socket Stream (1/1) (attempt #0) with attempt id 73b455378825552a61c8427db7ab7eea to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id a245cfd2a749df92a15769eca12db2be]
[INFO][2021-03-07 01:28:49 415][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, user_info]], fields=[id, name]) (1/1) (85e804a9121b4dc55cd3dd8ad7c56dd3) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 415][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Source: TableSourceScan(table=[[default_catalog, default_database, user_info]], fields=[id, name]) (1/1) (attempt #0) with attempt id 85e804a9121b4dc55cd3dd8ad7c56dd3 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id a245cfd2a749df92a15769eca12db2be]
[INFO][2021-03-07 01:28:49 417][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot a245cfd2a749df92a15769eca12db2be.]
[INFO][2021-03-07 01:28:49 417][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (1/12) (45ff3c022c3bd33d2c80ee20d5052231) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 418][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (1/12) (attempt #0) with attempt id 45ff3c022c3bd33d2c80ee20d5052231 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id a245cfd2a749df92a15769eca12db2be]
[INFO][2021-03-07 01:28:49 419][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (2/12) (bef978f6d83df54cc3c6ac6c1cdaca5a) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 419][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (2/12) (attempt #0) with attempt id bef978f6d83df54cc3c6ac6c1cdaca5a to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 25496b8033d98de9be27839608eee066]
[INFO][2021-03-07 01:28:49 419][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (3/12) (fc1e0100f1ada3986192b689441dbbdf) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 420][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (3/12) (attempt #0) with attempt id fc1e0100f1ada3986192b689441dbbdf to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 7b41d4e42a57905538a862dc6f41377f]
[INFO][2021-03-07 01:28:49 421][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (4/12) (ac375608c706165d2a3af65c6877c6cc) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 421][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (4/12) (attempt #0) with attempt id ac375608c706165d2a3af65c6877c6cc to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id f8689d61dda41c6ded47c656789b3051]
[INFO][2021-03-07 01:28:49 421][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (5/12) (c3c4c25f49779d48a8c790a1681bf92e) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 421][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (5/12) (attempt #0) with attempt id c3c4c25f49779d48a8c790a1681bf92e to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 3e792c09e4a4b1f17340028d3f492c0b]
[INFO][2021-03-07 01:28:49 421][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (6/12) (1f242ff02c00f8a6380431415e2796d4) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 421][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (6/12) (attempt #0) with attempt id 1f242ff02c00f8a6380431415e2796d4 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 385c2b39c682065ee1b01a0d53a04242]
[INFO][2021-03-07 01:28:49 422][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (7/12) (f725beb86a62a0bd926948e592b4495a) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 422][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (7/12) (attempt #0) with attempt id f725beb86a62a0bd926948e592b4495a to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id b16595f537326ab9da08466aa47563d3]
[INFO][2021-03-07 01:28:49 422][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (8/12) (2b9dce7547f49f7a4fe30fd1488ecc63) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 422][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (8/12) (attempt #0) with attempt id 2b9dce7547f49f7a4fe30fd1488ecc63 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 3a91bdb43645961c87c8fd500ff54728]
[INFO][2021-03-07 01:28:49 423][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (9/12) (bfa16f9c3bcde04c228561ea67f59a25) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 423][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (9/12) (attempt #0) with attempt id bfa16f9c3bcde04c228561ea67f59a25 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id ce26798dd40fb2d0655b066bd1a24372]
[INFO][2021-03-07 01:28:49 423][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (10/12) (5d646f8fed361a4ebc98c81799f75c29) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 423][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (10/12) (attempt #0) with attempt id 5d646f8fed361a4ebc98c81799f75c29 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id eacb317cb275b479e8fcbbb3ea7016a6]
[INFO][2021-03-07 01:28:49 423][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (11/12) (c36f74113749b8ab945e42d2a3412453) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 423][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (11/12) (attempt #0) with attempt id c36f74113749b8ab945e42d2a3412453 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 09e3a764b09fbbbc3e7b5d94131ca3cd]
[INFO][2021-03-07 01:28:49 423][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (12/12) (bd57d9f1a749653d29aca9f59ba8b220) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 423][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (12/12) (attempt #0) with attempt id bd57d9f1a749653d29aca9f59ba8b220 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 76b0a8587b580146382d96f4d2db67cd]
[INFO][2021-03-07 01:28:49 424][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (1/12) (d68a4b0e748519771063b5d971bf482f) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 424][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (1/12) (attempt #0) with attempt id d68a4b0e748519771063b5d971bf482f to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id a245cfd2a749df92a15769eca12db2be]
[INFO][2021-03-07 01:28:49 425][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (2/12) (99543f7c156cd9852dde059f337dd602) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 426][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (2/12) (attempt #0) with attempt id 99543f7c156cd9852dde059f337dd602 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 25496b8033d98de9be27839608eee066]
[INFO][2021-03-07 01:28:49 426][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (3/12) (a667de01414f7db1404cb6cb0ee37a05) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 426][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (3/12) (attempt #0) with attempt id a667de01414f7db1404cb6cb0ee37a05 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 7b41d4e42a57905538a862dc6f41377f]
[INFO][2021-03-07 01:28:49 426][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (4/12) (b4d8efc5f3f22e08b27c6219636b9fce) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 426][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (4/12) (attempt #0) with attempt id b4d8efc5f3f22e08b27c6219636b9fce to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id f8689d61dda41c6ded47c656789b3051]
[INFO][2021-03-07 01:28:49 426][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (5/12) (ff48db4a5db1743dc62c918a8fd13ea5) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 426][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (5/12) (attempt #0) with attempt id ff48db4a5db1743dc62c918a8fd13ea5 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 3e792c09e4a4b1f17340028d3f492c0b]
[INFO][2021-03-07 01:28:49 426][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (6/12) (63e2da992a8707cc702003c12ae8fc47) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 426][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (6/12) (attempt #0) with attempt id 63e2da992a8707cc702003c12ae8fc47 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 385c2b39c682065ee1b01a0d53a04242]
[INFO][2021-03-07 01:28:49 427][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (7/12) (9928e0be325db8629c9d508cd2b437e7) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 427][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (7/12) (attempt #0) with attempt id 9928e0be325db8629c9d508cd2b437e7 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id b16595f537326ab9da08466aa47563d3]
[INFO][2021-03-07 01:28:49 427][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (8/12) (14a4e27517cf1a7b8e7cc2c5be6b4175) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 427][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (8/12) (attempt #0) with attempt id 14a4e27517cf1a7b8e7cc2c5be6b4175 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 3a91bdb43645961c87c8fd500ff54728]
[INFO][2021-03-07 01:28:49 427][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (9/12) (519976251dfcda4d4f597b5c575d4121) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 427][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (9/12) (attempt #0) with attempt id 519976251dfcda4d4f597b5c575d4121 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id ce26798dd40fb2d0655b066bd1a24372]
[INFO][2021-03-07 01:28:49 427][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (10/12) (f807ff0c3cf1d8e4b24931ac9d689322) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 427][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (10/12) (attempt #0) with attempt id f807ff0c3cf1d8e4b24931ac9d689322 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id eacb317cb275b479e8fcbbb3ea7016a6]
[INFO][2021-03-07 01:28:49 427][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (11/12) (5711d7bb4415778dccdf25165eb69bb2) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 428][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (11/12) (attempt #0) with attempt id 5711d7bb4415778dccdf25165eb69bb2 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 09e3a764b09fbbbc3e7b5d94131ca3cd]
[INFO][2021-03-07 01:28:49 428][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (12/12) (cc1d2fc55dfdb578697413caeb92575d) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 428][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (12/12) (attempt #0) with attempt id cc1d2fc55dfdb578697413caeb92575d to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 76b0a8587b580146382d96f4d2db67cd]
[INFO][2021-03-07 01:28:49 428][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (1/12) (117d90b31e9303984af64aeb0ee12b7b) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 428][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (1/12) (attempt #0) with attempt id 117d90b31e9303984af64aeb0ee12b7b to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id a245cfd2a749df92a15769eca12db2be]
[INFO][2021-03-07 01:28:49 429][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (2/12) (46eb3e3a2642f2cef32223cd9639bafb) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 429][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (2/12) (attempt #0) with attempt id 46eb3e3a2642f2cef32223cd9639bafb to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 25496b8033d98de9be27839608eee066]
[INFO][2021-03-07 01:28:49 429][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (3/12) (8313e5af712d2045593482a0820cc361) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 429][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (3/12) (attempt #0) with attempt id 8313e5af712d2045593482a0820cc361 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 7b41d4e42a57905538a862dc6f41377f]
[INFO][2021-03-07 01:28:49 429][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (4/12) (434250ba6208041fd62e2a353b104611) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 429][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (4/12) (attempt #0) with attempt id 434250ba6208041fd62e2a353b104611 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id f8689d61dda41c6ded47c656789b3051]
[INFO][2021-03-07 01:28:49 430][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (5/12) (73c20f4a24318650c5d7f912965d81cd) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 430][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (5/12) (attempt #0) with attempt id 73c20f4a24318650c5d7f912965d81cd to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 3e792c09e4a4b1f17340028d3f492c0b]
[INFO][2021-03-07 01:28:49 430][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (6/12) (9c7f4cc7e9397dca962563d379b6807b) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 430][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (6/12) (attempt #0) with attempt id 9c7f4cc7e9397dca962563d379b6807b to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 385c2b39c682065ee1b01a0d53a04242]
[INFO][2021-03-07 01:28:49 430][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (7/12) (f8b2448dfa727dda6a93c0ad0d703d2b) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 430][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (7/12) (attempt #0) with attempt id f8b2448dfa727dda6a93c0ad0d703d2b to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id b16595f537326ab9da08466aa47563d3]
[INFO][2021-03-07 01:28:49 431][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (8/12) (9969099a68f795d11bfa385055caa659) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 431][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (8/12) (attempt #0) with attempt id 9969099a68f795d11bfa385055caa659 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 3a91bdb43645961c87c8fd500ff54728]
[INFO][2021-03-07 01:28:49 431][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (9/12) (d0b9957e0c5be6e650276ff20a50006e) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 431][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (9/12) (attempt #0) with attempt id d0b9957e0c5be6e650276ff20a50006e to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id ce26798dd40fb2d0655b066bd1a24372]
[INFO][2021-03-07 01:28:49 432][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (10/12) (e191529b5423345b14e740a793696c8c) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 432][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (10/12) (attempt #0) with attempt id e191529b5423345b14e740a793696c8c to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id eacb317cb275b479e8fcbbb3ea7016a6]
[INFO][2021-03-07 01:28:49 432][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (11/12) (9db6958e7070d1ddd8dc0ff744ee978b) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 432][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (11/12) (attempt #0) with attempt id 9db6958e7070d1ddd8dc0ff744ee978b to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 09e3a764b09fbbbc3e7b5d94131ca3cd]
[INFO][2021-03-07 01:28:49 432][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (12/12) (3be3b9e9654c94309f3c9da98452d53d) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 432][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (12/12) (attempt #0) with attempt id 3be3b9e9654c94309f3c9da98452d53d to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 76b0a8587b580146382d96f4d2db67cd]
[INFO][2021-03-07 01:28:49 433][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (1/12) (3a2ad6257fb3a80bcb405ae55a722bf9) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 433][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (1/12) (attempt #0) with attempt id 3a2ad6257fb3a80bcb405ae55a722bf9 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id a245cfd2a749df92a15769eca12db2be]
[INFO][2021-03-07 01:28:49 433][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (2/12) (c04c3ad746a573401a119e7890251457) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 433][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (2/12) (attempt #0) with attempt id c04c3ad746a573401a119e7890251457 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 25496b8033d98de9be27839608eee066]
[INFO][2021-03-07 01:28:49 434][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (3/12) (49993121b2bae42027be4a7a79e9f137) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 434][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (3/12) (attempt #0) with attempt id 49993121b2bae42027be4a7a79e9f137 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 7b41d4e42a57905538a862dc6f41377f]
[INFO][2021-03-07 01:28:49 434][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (4/12) (b99fac6c26d8d31cd214b70ede5f8408) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 434][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (4/12) (attempt #0) with attempt id b99fac6c26d8d31cd214b70ede5f8408 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id f8689d61dda41c6ded47c656789b3051]
[INFO][2021-03-07 01:28:49 434][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (5/12) (a972e3542a745aaface3f6884d3be146) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 434][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (5/12) (attempt #0) with attempt id a972e3542a745aaface3f6884d3be146 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 3e792c09e4a4b1f17340028d3f492c0b]
[INFO][2021-03-07 01:28:49 434][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (6/12) (303003e33bea497b92f3c6b4ed1fa944) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 434][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (6/12) (attempt #0) with attempt id 303003e33bea497b92f3c6b4ed1fa944 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 385c2b39c682065ee1b01a0d53a04242]
[INFO][2021-03-07 01:28:49 434][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (7/12) (979d770095d912b2cf79c375665d892b) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 434][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (7/12) (attempt #0) with attempt id 979d770095d912b2cf79c375665d892b to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id b16595f537326ab9da08466aa47563d3]
[INFO][2021-03-07 01:28:49 435][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (8/12) (f8b66729777ee0b7da16ae8c8bca4842) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 435][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (8/12) (attempt #0) with attempt id f8b66729777ee0b7da16ae8c8bca4842 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 3a91bdb43645961c87c8fd500ff54728]
[INFO][2021-03-07 01:28:49 435][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (9/12) (3adb7e604c13ae04b4bfe0b9a929e56f) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 435][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (9/12) (attempt #0) with attempt id 3adb7e604c13ae04b4bfe0b9a929e56f to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id ce26798dd40fb2d0655b066bd1a24372]
[INFO][2021-03-07 01:28:49 435][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (10/12) (2e7d0f61fa31884d84a63807a51288fd) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 435][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (10/12) (attempt #0) with attempt id 2e7d0f61fa31884d84a63807a51288fd to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id eacb317cb275b479e8fcbbb3ea7016a6]
[INFO][2021-03-07 01:28:49 435][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (11/12) (b8a21e1f20db61e56e6d176422664e9f) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 435][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (11/12) (attempt #0) with attempt id b8a21e1f20db61e56e6d176422664e9f to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 09e3a764b09fbbbc3e7b5d94131ca3cd]
[INFO][2021-03-07 01:28:49 436][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (12/12) (32f2fb6525d5156158194b6f80112bb4) switched from SCHEDULED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 436][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Deploying Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (12/12) (attempt #0) with attempt id 32f2fb6525d5156158194b6f80112bb4 to 1a228b00-0115-4502-8d76-6df23e70baeb @ localhost (dataPort=-1) with allocation id 76b0a8587b580146382d96f4d2db67cd]
[INFO][2021-03-07 01:28:49 464][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Source: Socket Stream (1/1)#0 (73b455378825552a61c8427db7ab7eea), deploy into slot with allocation id a245cfd2a749df92a15769eca12db2be.]
[INFO][2021-03-07 01:28:49 465][org.apache.flink.runtime.taskmanager.Task]-[Source: Socket Stream (1/1)#0 (73b455378825552a61c8427db7ab7eea) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 467][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot a245cfd2a749df92a15769eca12db2be.]
[INFO][2021-03-07 01:28:49 469][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Source: TableSourceScan(table=[[default_catalog, default_database, user_info]], fields=[id, name]) (1/1)#0 (85e804a9121b4dc55cd3dd8ad7c56dd3), deploy into slot with allocation id a245cfd2a749df92a15769eca12db2be.]
[INFO][2021-03-07 01:28:49 470][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, user_info]], fields=[id, name]) (1/1)#0 (85e804a9121b4dc55cd3dd8ad7c56dd3) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 471][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot a245cfd2a749df92a15769eca12db2be.]
[INFO][2021-03-07 01:28:49 472][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Source: Socket Stream (1/1)#0 (73b455378825552a61c8427db7ab7eea) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 472][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, default_database, user_info]], fields=[id, name]) (1/1)#0 (85e804a9121b4dc55cd3dd8ad7c56dd3) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 473][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (1/12)#0 (45ff3c022c3bd33d2c80ee20d5052231), deploy into slot with allocation id a245cfd2a749df92a15769eca12db2be.]
[INFO][2021-03-07 01:28:49 473][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Source: Socket Stream (1/1)#0 (73b455378825552a61c8427db7ab7eea) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 473][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Source: TableSourceScan(table=[[default_catalog, default_database, user_info]], fields=[id, name]) (1/1)#0 (85e804a9121b4dc55cd3dd8ad7c56dd3) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 473][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (1/12)#0 (45ff3c022c3bd33d2c80ee20d5052231) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 473][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (1/12)#0 (45ff3c022c3bd33d2c80ee20d5052231) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 473][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 25496b8033d98de9be27839608eee066.]
[INFO][2021-03-07 01:28:49 474][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (1/12)#0 (45ff3c022c3bd33d2c80ee20d5052231) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 475][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (2/12)#0 (bef978f6d83df54cc3c6ac6c1cdaca5a), deploy into slot with allocation id 25496b8033d98de9be27839608eee066.]
[INFO][2021-03-07 01:28:49 476][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (2/12)#0 (bef978f6d83df54cc3c6ac6c1cdaca5a) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 476][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 7b41d4e42a57905538a862dc6f41377f.]
[INFO][2021-03-07 01:28:49 476][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (2/12)#0 (bef978f6d83df54cc3c6ac6c1cdaca5a) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 477][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (2/12)#0 (bef978f6d83df54cc3c6ac6c1cdaca5a) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 479][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (3/12)#0 (fc1e0100f1ada3986192b689441dbbdf), deploy into slot with allocation id 7b41d4e42a57905538a862dc6f41377f.]
[INFO][2021-03-07 01:28:49 480][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (3/12)#0 (fc1e0100f1ada3986192b689441dbbdf) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 480][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (3/12)#0 (fc1e0100f1ada3986192b689441dbbdf) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 480][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot f8689d61dda41c6ded47c656789b3051.]
[INFO][2021-03-07 01:28:49 480][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (3/12)#0 (fc1e0100f1ada3986192b689441dbbdf) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 483][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (4/12)#0 (ac375608c706165d2a3af65c6877c6cc), deploy into slot with allocation id f8689d61dda41c6ded47c656789b3051.]
[INFO][2021-03-07 01:28:49 483][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (4/12)#0 (ac375608c706165d2a3af65c6877c6cc) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 483][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 3e792c09e4a4b1f17340028d3f492c0b.]
[INFO][2021-03-07 01:28:49 483][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (4/12)#0 (ac375608c706165d2a3af65c6877c6cc) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 484][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (4/12)#0 (ac375608c706165d2a3af65c6877c6cc) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 485][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (5/12)#0 (c3c4c25f49779d48a8c790a1681bf92e), deploy into slot with allocation id 3e792c09e4a4b1f17340028d3f492c0b.]
[INFO][2021-03-07 01:28:49 485][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (5/12)#0 (c3c4c25f49779d48a8c790a1681bf92e) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 485][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 385c2b39c682065ee1b01a0d53a04242.]
[INFO][2021-03-07 01:28:49 485][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (5/12)#0 (c3c4c25f49779d48a8c790a1681bf92e) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 486][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (5/12)#0 (c3c4c25f49779d48a8c790a1681bf92e) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 488][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (6/12)#0 (1f242ff02c00f8a6380431415e2796d4), deploy into slot with allocation id 385c2b39c682065ee1b01a0d53a04242.]
[INFO][2021-03-07 01:28:49 489][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (6/12)#0 (1f242ff02c00f8a6380431415e2796d4) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 489][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (6/12)#0 (1f242ff02c00f8a6380431415e2796d4) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 489][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot b16595f537326ab9da08466aa47563d3.]
[INFO][2021-03-07 01:28:49 489][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (6/12)#0 (1f242ff02c00f8a6380431415e2796d4) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 492][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (7/12)#0 (f725beb86a62a0bd926948e592b4495a), deploy into slot with allocation id b16595f537326ab9da08466aa47563d3.]
[INFO][2021-03-07 01:28:49 492][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (7/12)#0 (f725beb86a62a0bd926948e592b4495a) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 492][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (7/12)#0 (f725beb86a62a0bd926948e592b4495a) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 492][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 3a91bdb43645961c87c8fd500ff54728.]
[INFO][2021-03-07 01:28:49 493][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (7/12)#0 (f725beb86a62a0bd926948e592b4495a) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 495][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (8/12)#0 (2b9dce7547f49f7a4fe30fd1488ecc63), deploy into slot with allocation id 3a91bdb43645961c87c8fd500ff54728.]
[INFO][2021-03-07 01:28:49 495][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (8/12)#0 (2b9dce7547f49f7a4fe30fd1488ecc63) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 495][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (8/12)#0 (2b9dce7547f49f7a4fe30fd1488ecc63) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 495][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot ce26798dd40fb2d0655b066bd1a24372.]
[INFO][2021-03-07 01:28:49 496][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (8/12)#0 (2b9dce7547f49f7a4fe30fd1488ecc63) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 499][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (9/12)#0 (bfa16f9c3bcde04c228561ea67f59a25), deploy into slot with allocation id ce26798dd40fb2d0655b066bd1a24372.]
[INFO][2021-03-07 01:28:49 500][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (9/12)#0 (bfa16f9c3bcde04c228561ea67f59a25) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 501][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (9/12)#0 (bfa16f9c3bcde04c228561ea67f59a25) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 501][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot eacb317cb275b479e8fcbbb3ea7016a6.]
[INFO][2021-03-07 01:28:49 501][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (9/12)#0 (bfa16f9c3bcde04c228561ea67f59a25) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 512][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (10/12)#0 (5d646f8fed361a4ebc98c81799f75c29), deploy into slot with allocation id eacb317cb275b479e8fcbbb3ea7016a6.]
[INFO][2021-03-07 01:28:49 512][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (10/12)#0 (5d646f8fed361a4ebc98c81799f75c29) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 512][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (10/12)#0 (5d646f8fed361a4ebc98c81799f75c29) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 512][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 09e3a764b09fbbbc3e7b5d94131ca3cd.]
[INFO][2021-03-07 01:28:49 513][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (10/12)#0 (5d646f8fed361a4ebc98c81799f75c29) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 515][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (11/12)#0 (c36f74113749b8ab945e42d2a3412453), deploy into slot with allocation id 09e3a764b09fbbbc3e7b5d94131ca3cd.]
[INFO][2021-03-07 01:28:49 516][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (11/12)#0 (c36f74113749b8ab945e42d2a3412453) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 516][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 76b0a8587b580146382d96f4d2db67cd.]
[INFO][2021-03-07 01:28:49 516][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (11/12)#0 (c36f74113749b8ab945e42d2a3412453) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 517][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (11/12)#0 (c36f74113749b8ab945e42d2a3412453) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 518][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 518][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 518][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 518][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 518][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 518][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 518][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 518][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 518][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 518][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 518][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 518][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 519][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 519][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (12/12)#0 (bd57d9f1a749653d29aca9f59ba8b220), deploy into slot with allocation id 76b0a8587b580146382d96f4d2db67cd.]
[INFO][2021-03-07 01:28:49 519][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (12/12)#0 (bd57d9f1a749653d29aca9f59ba8b220) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 520][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot a245cfd2a749df92a15769eca12db2be.]
[INFO][2021-03-07 01:28:49 520][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (12/12)#0 (bd57d9f1a749653d29aca9f59ba8b220) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 520][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (12/12)#0 (bd57d9f1a749653d29aca9f59ba8b220) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 523][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 527][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (2/12)#0 (bef978f6d83df54cc3c6ac6c1cdaca5a) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 527][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, user_info]], fields=[id, name]) (1/1)#0 (85e804a9121b4dc55cd3dd8ad7c56dd3) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 527][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (6/12)#0 (1f242ff02c00f8a6380431415e2796d4) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 527][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (4/12)#0 (ac375608c706165d2a3af65c6877c6cc) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 527][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (5/12)#0 (c3c4c25f49779d48a8c790a1681bf92e) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 527][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (1/12)#0 (45ff3c022c3bd33d2c80ee20d5052231) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 527][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (12/12)#0 (bd57d9f1a749653d29aca9f59ba8b220) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 527][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (3/12)#0 (fc1e0100f1ada3986192b689441dbbdf) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 527][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (7/12)#0 (f725beb86a62a0bd926948e592b4495a) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 527][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (9/12)#0 (bfa16f9c3bcde04c228561ea67f59a25) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 527][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (8/12)#0 (2b9dce7547f49f7a4fe30fd1488ecc63) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 527][org.apache.flink.runtime.taskmanager.Task]-[Source: Socket Stream (1/1)#0 (73b455378825552a61c8427db7ab7eea) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 527][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (10/12)#0 (5d646f8fed361a4ebc98c81799f75c29) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 527][org.apache.flink.runtime.taskmanager.Task]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (11/12)#0 (c36f74113749b8ab945e42d2a3412453) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 528][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, user_info]], fields=[id, name]) (1/1) (85e804a9121b4dc55cd3dd8ad7c56dd3) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 529][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (2/12) (bef978f6d83df54cc3c6ac6c1cdaca5a) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 529][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (6/12) (1f242ff02c00f8a6380431415e2796d4) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 529][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (4/12) (ac375608c706165d2a3af65c6877c6cc) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 529][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (5/12) (c3c4c25f49779d48a8c790a1681bf92e) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 529][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (1/12) (45ff3c022c3bd33d2c80ee20d5052231) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 529][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (12/12) (bd57d9f1a749653d29aca9f59ba8b220) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 529][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (3/12) (fc1e0100f1ada3986192b689441dbbdf) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 529][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (7/12) (f725beb86a62a0bd926948e592b4495a) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 529][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (9/12) (bfa16f9c3bcde04c228561ea67f59a25) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 530][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (8/12) (2b9dce7547f49f7a4fe30fd1488ecc63) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 530][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: Socket Stream (1/1) (73b455378825552a61c8427db7ab7eea) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 530][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (10/12) (5d646f8fed361a4ebc98c81799f75c29) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 530][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) -> Calc(select=[orderName, id]) (11/12) (c36f74113749b8ab945e42d2a3412453) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 535][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (1/12)#0 (d68a4b0e748519771063b5d971bf482f), deploy into slot with allocation id a245cfd2a749df92a15769eca12db2be.]
[INFO][2021-03-07 01:28:49 535][org.apache.flink.runtime.taskmanager.Task]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (1/12)#0 (d68a4b0e748519771063b5d971bf482f) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 535][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (1/12)#0 (d68a4b0e748519771063b5d971bf482f) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 535][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 25496b8033d98de9be27839608eee066.]
[INFO][2021-03-07 01:28:49 536][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (1/12)#0 (d68a4b0e748519771063b5d971bf482f) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 538][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (2/12)#0 (99543f7c156cd9852dde059f337dd602), deploy into slot with allocation id 25496b8033d98de9be27839608eee066.]
[INFO][2021-03-07 01:28:49 539][org.apache.flink.runtime.taskmanager.Task]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (2/12)#0 (99543f7c156cd9852dde059f337dd602) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 539][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (2/12)#0 (99543f7c156cd9852dde059f337dd602) [DEPLOYING].]
[WARN][2021-03-07 01:28:49 539][org.apache.flink.metrics.MetricGroup]-[The operator name Source: TableSourceScan(table=[[default_catalog, default_database, user_info]], fields=[id, name]) exceeded the 80 characters length limit and was truncated.]
[INFO][2021-03-07 01:28:49 540][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 7b41d4e42a57905538a862dc6f41377f.]
[INFO][2021-03-07 01:28:49 540][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (2/12)#0 (99543f7c156cd9852dde059f337dd602) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 542][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (3/12)#0 (a667de01414f7db1404cb6cb0ee37a05), deploy into slot with allocation id 7b41d4e42a57905538a862dc6f41377f.]
[INFO][2021-03-07 01:28:49 543][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot f8689d61dda41c6ded47c656789b3051.]
[INFO][2021-03-07 01:28:49 543][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 543][org.apache.flink.runtime.taskmanager.Task]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (3/12)#0 (a667de01414f7db1404cb6cb0ee37a05) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 544][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (3/12)#0 (a667de01414f7db1404cb6cb0ee37a05) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 543][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 544][org.apache.flink.runtime.taskmanager.Task]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (1/12)#0 (d68a4b0e748519771063b5d971bf482f) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 544][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (3/12)#0 (a667de01414f7db1404cb6cb0ee37a05) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 545][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (1/12) (d68a4b0e748519771063b5d971bf482f) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 544][org.apache.flink.runtime.taskmanager.Task]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (2/12)#0 (99543f7c156cd9852dde059f337dd602) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 548][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (2/12) (99543f7c156cd9852dde059f337dd602) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 549][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (4/12)#0 (b4d8efc5f3f22e08b27c6219636b9fce), deploy into slot with allocation id f8689d61dda41c6ded47c656789b3051.]
[INFO][2021-03-07 01:28:49 549][org.apache.flink.runtime.taskmanager.Task]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (4/12)#0 (b4d8efc5f3f22e08b27c6219636b9fce) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 549][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (4/12)#0 (b4d8efc5f3f22e08b27c6219636b9fce) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 549][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 3e792c09e4a4b1f17340028d3f492c0b.]
[INFO][2021-03-07 01:28:49 550][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (4/12)#0 (b4d8efc5f3f22e08b27c6219636b9fce) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 550][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 551][org.apache.flink.runtime.taskmanager.Task]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (3/12)#0 (a667de01414f7db1404cb6cb0ee37a05) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 553][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (3/12) (a667de01414f7db1404cb6cb0ee37a05) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 553][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (5/12)#0 (ff48db4a5db1743dc62c918a8fd13ea5), deploy into slot with allocation id 3e792c09e4a4b1f17340028d3f492c0b.]
[INFO][2021-03-07 01:28:49 554][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 385c2b39c682065ee1b01a0d53a04242.]
[INFO][2021-03-07 01:28:49 554][org.apache.flink.runtime.taskmanager.Task]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (5/12)#0 (ff48db4a5db1743dc62c918a8fd13ea5) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 554][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (5/12)#0 (ff48db4a5db1743dc62c918a8fd13ea5) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 555][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (5/12)#0 (ff48db4a5db1743dc62c918a8fd13ea5) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 557][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 557][org.apache.flink.runtime.taskmanager.Task]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (4/12)#0 (b4d8efc5f3f22e08b27c6219636b9fce) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 557][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (4/12) (b4d8efc5f3f22e08b27c6219636b9fce) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 558][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (6/12)#0 (63e2da992a8707cc702003c12ae8fc47), deploy into slot with allocation id 385c2b39c682065ee1b01a0d53a04242.]
[INFO][2021-03-07 01:28:49 558][org.apache.flink.runtime.taskmanager.Task]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (6/12)#0 (63e2da992a8707cc702003c12ae8fc47) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 558][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 558][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot b16595f537326ab9da08466aa47563d3.]
[INFO][2021-03-07 01:28:49 558][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (6/12)#0 (63e2da992a8707cc702003c12ae8fc47) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 558][org.apache.flink.runtime.taskmanager.Task]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (5/12)#0 (ff48db4a5db1743dc62c918a8fd13ea5) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 558][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (5/12) (ff48db4a5db1743dc62c918a8fd13ea5) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 559][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (6/12)#0 (63e2da992a8707cc702003c12ae8fc47) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 560][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (7/12)#0 (9928e0be325db8629c9d508cd2b437e7), deploy into slot with allocation id b16595f537326ab9da08466aa47563d3.]
[INFO][2021-03-07 01:28:49 561][org.apache.flink.runtime.taskmanager.Task]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (7/12)#0 (9928e0be325db8629c9d508cd2b437e7) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 561][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 3a91bdb43645961c87c8fd500ff54728.]
[INFO][2021-03-07 01:28:49 561][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (7/12)#0 (9928e0be325db8629c9d508cd2b437e7) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 562][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (7/12)#0 (9928e0be325db8629c9d508cd2b437e7) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 562][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 562][org.apache.flink.runtime.taskmanager.Task]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (6/12)#0 (63e2da992a8707cc702003c12ae8fc47) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 563][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (6/12) (63e2da992a8707cc702003c12ae8fc47) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 564][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (8/12)#0 (14a4e27517cf1a7b8e7cc2c5be6b4175), deploy into slot with allocation id 3a91bdb43645961c87c8fd500ff54728.]
[INFO][2021-03-07 01:28:49 564][org.apache.flink.runtime.taskmanager.Task]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (8/12)#0 (14a4e27517cf1a7b8e7cc2c5be6b4175) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 564][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot ce26798dd40fb2d0655b066bd1a24372.]
[INFO][2021-03-07 01:28:49 564][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (8/12)#0 (14a4e27517cf1a7b8e7cc2c5be6b4175) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 565][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (8/12)#0 (14a4e27517cf1a7b8e7cc2c5be6b4175) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 565][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 565][org.apache.flink.runtime.taskmanager.Task]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (7/12)#0 (9928e0be325db8629c9d508cd2b437e7) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 566][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (7/12) (9928e0be325db8629c9d508cd2b437e7) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 566][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (9/12)#0 (519976251dfcda4d4f597b5c575d4121), deploy into slot with allocation id ce26798dd40fb2d0655b066bd1a24372.]
[INFO][2021-03-07 01:28:49 567][org.apache.flink.runtime.taskmanager.Task]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (9/12)#0 (519976251dfcda4d4f597b5c575d4121) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 567][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot eacb317cb275b479e8fcbbb3ea7016a6.]
[INFO][2021-03-07 01:28:49 567][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (9/12)#0 (519976251dfcda4d4f597b5c575d4121) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 567][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (9/12)#0 (519976251dfcda4d4f597b5c575d4121) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 568][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 568][org.apache.flink.runtime.taskmanager.Task]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (8/12)#0 (14a4e27517cf1a7b8e7cc2c5be6b4175) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 569][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (8/12) (14a4e27517cf1a7b8e7cc2c5be6b4175) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 569][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (10/12)#0 (f807ff0c3cf1d8e4b24931ac9d689322), deploy into slot with allocation id eacb317cb275b479e8fcbbb3ea7016a6.]
[INFO][2021-03-07 01:28:49 569][org.apache.flink.runtime.taskmanager.Task]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (10/12)#0 (f807ff0c3cf1d8e4b24931ac9d689322) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 569][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 09e3a764b09fbbbc3e7b5d94131ca3cd.]
[INFO][2021-03-07 01:28:49 569][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (10/12)#0 (f807ff0c3cf1d8e4b24931ac9d689322) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 570][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (10/12)#0 (f807ff0c3cf1d8e4b24931ac9d689322) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 570][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 571][org.apache.flink.runtime.taskmanager.Task]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (9/12)#0 (519976251dfcda4d4f597b5c575d4121) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 571][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (9/12) (519976251dfcda4d4f597b5c575d4121) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 571][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (11/12)#0 (5711d7bb4415778dccdf25165eb69bb2), deploy into slot with allocation id 09e3a764b09fbbbc3e7b5d94131ca3cd.]
[INFO][2021-03-07 01:28:49 571][org.apache.flink.runtime.taskmanager.Task]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (11/12)#0 (5711d7bb4415778dccdf25165eb69bb2) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 571][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (11/12)#0 (5711d7bb4415778dccdf25165eb69bb2) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 571][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 76b0a8587b580146382d96f4d2db67cd.]
[INFO][2021-03-07 01:28:49 572][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (11/12)#0 (5711d7bb4415778dccdf25165eb69bb2) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 573][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 573][org.apache.flink.runtime.taskmanager.Task]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (10/12)#0 (f807ff0c3cf1d8e4b24931ac9d689322) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 573][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (10/12) (f807ff0c3cf1d8e4b24931ac9d689322) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 574][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (12/12)#0 (cc1d2fc55dfdb578697413caeb92575d), deploy into slot with allocation id 76b0a8587b580146382d96f4d2db67cd.]
[INFO][2021-03-07 01:28:49 574][org.apache.flink.runtime.taskmanager.Task]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (12/12)#0 (cc1d2fc55dfdb578697413caeb92575d) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 574][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (12/12)#0 (cc1d2fc55dfdb578697413caeb92575d) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 574][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot a245cfd2a749df92a15769eca12db2be.]
[INFO][2021-03-07 01:28:49 575][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (12/12)#0 (cc1d2fc55dfdb578697413caeb92575d) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 575][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 575][org.apache.flink.runtime.taskmanager.Task]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (11/12)#0 (5711d7bb4415778dccdf25165eb69bb2) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 576][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (11/12) (5711d7bb4415778dccdf25165eb69bb2) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 577][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (1/12)#0 (117d90b31e9303984af64aeb0ee12b7b), deploy into slot with allocation id a245cfd2a749df92a15769eca12db2be.]
[INFO][2021-03-07 01:28:49 577][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (1/12)#0 (117d90b31e9303984af64aeb0ee12b7b) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 577][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 25496b8033d98de9be27839608eee066.]
[INFO][2021-03-07 01:28:49 577][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (1/12)#0 (117d90b31e9303984af64aeb0ee12b7b) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 578][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (1/12)#0 (117d90b31e9303984af64aeb0ee12b7b) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 578][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 578][org.apache.flink.runtime.taskmanager.Task]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (12/12)#0 (cc1d2fc55dfdb578697413caeb92575d) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 580][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Map -> SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) (12/12) (cc1d2fc55dfdb578697413caeb92575d) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 580][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (2/12)#0 (46eb3e3a2642f2cef32223cd9639bafb), deploy into slot with allocation id 25496b8033d98de9be27839608eee066.]
[INFO][2021-03-07 01:28:49 580][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (2/12)#0 (46eb3e3a2642f2cef32223cd9639bafb) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 580][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 7b41d4e42a57905538a862dc6f41377f.]
[INFO][2021-03-07 01:28:49 580][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (2/12)#0 (46eb3e3a2642f2cef32223cd9639bafb) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 581][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (2/12)#0 (46eb3e3a2642f2cef32223cd9639bafb) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 582][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (3/12)#0 (8313e5af712d2045593482a0820cc361), deploy into slot with allocation id 7b41d4e42a57905538a862dc6f41377f.]
[INFO][2021-03-07 01:28:49 582][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 582][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (3/12)#0 (8313e5af712d2045593482a0820cc361) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 582][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot f8689d61dda41c6ded47c656789b3051.]
[INFO][2021-03-07 01:28:49 582][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (3/12)#0 (8313e5af712d2045593482a0820cc361) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 583][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (1/12)#0 (117d90b31e9303984af64aeb0ee12b7b) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 583][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (3/12)#0 (8313e5af712d2045593482a0820cc361) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 583][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 583][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (1/12) (117d90b31e9303984af64aeb0ee12b7b) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 583][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (2/12)#0 (46eb3e3a2642f2cef32223cd9639bafb) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 583][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (2/12) (46eb3e3a2642f2cef32223cd9639bafb) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 584][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (4/12)#0 (434250ba6208041fd62e2a353b104611), deploy into slot with allocation id f8689d61dda41c6ded47c656789b3051.]
[INFO][2021-03-07 01:28:49 584][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (4/12)#0 (434250ba6208041fd62e2a353b104611) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 584][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 3e792c09e4a4b1f17340028d3f492c0b.]
[INFO][2021-03-07 01:28:49 584][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (4/12)#0 (434250ba6208041fd62e2a353b104611) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 585][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (4/12)#0 (434250ba6208041fd62e2a353b104611) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 585][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 586][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (3/12)#0 (8313e5af712d2045593482a0820cc361) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 587][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (3/12) (8313e5af712d2045593482a0820cc361) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 587][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (5/12)#0 (73c20f4a24318650c5d7f912965d81cd), deploy into slot with allocation id 3e792c09e4a4b1f17340028d3f492c0b.]
[INFO][2021-03-07 01:28:49 587][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (5/12)#0 (73c20f4a24318650c5d7f912965d81cd) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 587][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 385c2b39c682065ee1b01a0d53a04242.]
[INFO][2021-03-07 01:28:49 587][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (5/12)#0 (73c20f4a24318650c5d7f912965d81cd) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 588][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 588][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (5/12)#0 (73c20f4a24318650c5d7f912965d81cd) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 588][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (4/12)#0 (434250ba6208041fd62e2a353b104611) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 588][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (4/12) (434250ba6208041fd62e2a353b104611) switched from DEPLOYING to RUNNING.]
[WARN][2021-03-07 01:28:49 588][org.apache.flink.metrics.MetricGroup]-[The operator name SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 588][org.apache.flink.metrics.MetricGroup]-[The operator name SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) exceeded the 80 characters length limit and was truncated.]
[INFO][2021-03-07 01:28:49 589][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (6/12)#0 (9c7f4cc7e9397dca962563d379b6807b), deploy into slot with allocation id 385c2b39c682065ee1b01a0d53a04242.]
[WARN][2021-03-07 01:28:49 589][org.apache.flink.metrics.MetricGroup]-[The operator name SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 589][org.apache.flink.metrics.MetricGroup]-[The operator name SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) exceeded the 80 characters length limit and was truncated.]
[INFO][2021-03-07 01:28:49 590][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (6/12)#0 (9c7f4cc7e9397dca962563d379b6807b) switched from CREATED to DEPLOYING.]
[WARN][2021-03-07 01:28:49 589][org.apache.flink.metrics.MetricGroup]-[The operator name SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 589][org.apache.flink.metrics.MetricGroup]-[The operator name SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 589][org.apache.flink.metrics.MetricGroup]-[The operator name SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 589][org.apache.flink.metrics.MetricGroup]-[The operator name SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 589][org.apache.flink.metrics.MetricGroup]-[The operator name SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 588][org.apache.flink.metrics.MetricGroup]-[The operator name SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 588][org.apache.flink.metrics.MetricGroup]-[The operator name SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 588][org.apache.flink.metrics.MetricGroup]-[The operator name SourceConversion(table=[default_catalog.default_database.be_table], fields=[userId, orderId]) exceeded the 80 characters length limit and was truncated.]
[INFO][2021-03-07 01:28:49 593][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 593][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (5/12)#0 (73c20f4a24318650c5d7f912965d81cd) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 593][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (6/12)#0 (9c7f4cc7e9397dca962563d379b6807b) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 591][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot b16595f537326ab9da08466aa47563d3.]
[INFO][2021-03-07 01:28:49 594][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (5/12) (73c20f4a24318650c5d7f912965d81cd) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 594][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (6/12)#0 (9c7f4cc7e9397dca962563d379b6807b) [DEPLOYING].]
[WARN][2021-03-07 01:28:49 595][org.apache.flink.metrics.MetricGroup]-[The operator name Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 595][org.apache.flink.metrics.MetricGroup]-[The operator name Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 595][org.apache.flink.metrics.MetricGroup]-[The operator name Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 595][org.apache.flink.metrics.MetricGroup]-[The operator name Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 595][org.apache.flink.metrics.MetricGroup]-[The operator name Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) exceeded the 80 characters length limit and was truncated.]
[INFO][2021-03-07 01:28:49 596][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 596][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (6/12)#0 (9c7f4cc7e9397dca962563d379b6807b) switched from DEPLOYING to RUNNING.]
[WARN][2021-03-07 01:28:49 597][org.apache.flink.metrics.MetricGroup]-[The operator name Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 597][org.apache.flink.metrics.MetricGroup]-[The operator name Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) exceeded the 80 characters length limit and was truncated.]
[INFO][2021-03-07 01:28:49 597][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (6/12) (9c7f4cc7e9397dca962563d379b6807b) switched from DEPLOYING to RUNNING.]
[WARN][2021-03-07 01:28:49 598][org.apache.flink.metrics.MetricGroup]-[The operator name Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 598][org.apache.flink.metrics.MetricGroup]-[The operator name Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 598][org.apache.flink.metrics.MetricGroup]-[The operator name Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 599][org.apache.flink.metrics.MetricGroup]-[The operator name Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 600][org.apache.flink.metrics.MetricGroup]-[The operator name Source: TableSourceScan(table=[[default_catalog, default_database, order_info, watermark=[-(CAST($3):TIMESTAMP(3), 2000:INTERVAL SECOND)]]], fields=[orderName, id, offset, timestamp]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 602][org.apache.flink.metrics.MetricGroup]-[The operator name Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) exceeded the 80 characters length limit and was truncated.]
[INFO][2021-03-07 01:28:49 603][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (7/12)#0 (f8b2448dfa727dda6a93c0ad0d703d2b), deploy into slot with allocation id b16595f537326ab9da08466aa47563d3.]
[WARN][2021-03-07 01:28:49 603][org.apache.flink.metrics.MetricGroup]-[The operator name Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 604][org.apache.flink.metrics.MetricGroup]-[The operator name Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) exceeded the 80 characters length limit and was truncated.]
[INFO][2021-03-07 01:28:49 604][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (7/12)#0 (f8b2448dfa727dda6a93c0ad0d703d2b) switched from CREATED to DEPLOYING.]
[WARN][2021-03-07 01:28:49 604][org.apache.flink.metrics.MetricGroup]-[The operator name Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) exceeded the 80 characters length limit and was truncated.]
[INFO][2021-03-07 01:28:49 604][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (7/12)#0 (f8b2448dfa727dda6a93c0ad0d703d2b) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 604][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 3a91bdb43645961c87c8fd500ff54728.]
[WARN][2021-03-07 01:28:49 604][org.apache.flink.metrics.MetricGroup]-[The operator name Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) exceeded the 80 characters length limit and was truncated.]
[INFO][2021-03-07 01:28:49 604][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (7/12)#0 (f8b2448dfa727dda6a93c0ad0d703d2b) [DEPLOYING].]
[WARN][2021-03-07 01:28:49 605][org.apache.flink.metrics.MetricGroup]-[The operator name Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) exceeded the 80 characters length limit and was truncated.]
[INFO][2021-03-07 01:28:49 607][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (8/12)#0 (9969099a68f795d11bfa385055caa659), deploy into slot with allocation id 3a91bdb43645961c87c8fd500ff54728.]
[INFO][2021-03-07 01:28:49 607][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (8/12)#0 (9969099a68f795d11bfa385055caa659) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 607][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 09e3a764b09fbbbc3e7b5d94131ca3cd.]
[INFO][2021-03-07 01:28:49 607][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (8/12)#0 (9969099a68f795d11bfa385055caa659) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 608][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 608][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (7/12)#0 (f8b2448dfa727dda6a93c0ad0d703d2b) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 608][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (8/12)#0 (9969099a68f795d11bfa385055caa659) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 609][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (7/12) (f8b2448dfa727dda6a93c0ad0d703d2b) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 609][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (11/12)#0 (9db6958e7070d1ddd8dc0ff744ee978b), deploy into slot with allocation id 09e3a764b09fbbbc3e7b5d94131ca3cd.]
[INFO][2021-03-07 01:28:49 610][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (11/12)#0 (9db6958e7070d1ddd8dc0ff744ee978b) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 610][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot eacb317cb275b479e8fcbbb3ea7016a6.]
[INFO][2021-03-07 01:28:49 611][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (11/12)#0 (9db6958e7070d1ddd8dc0ff744ee978b) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 612][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (11/12)#0 (9db6958e7070d1ddd8dc0ff744ee978b) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 612][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 612][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (10/12)#0 (e191529b5423345b14e740a793696c8c), deploy into slot with allocation id eacb317cb275b479e8fcbbb3ea7016a6.]
[INFO][2021-03-07 01:28:49 612][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (8/12)#0 (9969099a68f795d11bfa385055caa659) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 612][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (8/12) (9969099a68f795d11bfa385055caa659) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 612][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (10/12)#0 (e191529b5423345b14e740a793696c8c) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 612][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot ce26798dd40fb2d0655b066bd1a24372.]
[INFO][2021-03-07 01:28:49 612][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (10/12)#0 (e191529b5423345b14e740a793696c8c) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 614][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (10/12)#0 (e191529b5423345b14e740a793696c8c) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 615][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (9/12)#0 (d0b9957e0c5be6e650276ff20a50006e), deploy into slot with allocation id ce26798dd40fb2d0655b066bd1a24372.]
[WARN][2021-03-07 01:28:49 615][org.apache.flink.metrics.MetricGroup]-[The operator name Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) exceeded the 80 characters length limit and was truncated.]
[INFO][2021-03-07 01:28:49 615][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 76b0a8587b580146382d96f4d2db67cd.]
[INFO][2021-03-07 01:28:49 616][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (9/12)#0 (d0b9957e0c5be6e650276ff20a50006e) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 616][org.apache.flink.streaming.api.functions.source.SocketTextStreamFunction]-[Connecting to server socket node01:9999]
[INFO][2021-03-07 01:28:49 616][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 616][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (11/12)#0 (9db6958e7070d1ddd8dc0ff744ee978b) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 616][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (9/12)#0 (d0b9957e0c5be6e650276ff20a50006e) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 616][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 618][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (10/12)#0 (e191529b5423345b14e740a793696c8c) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 618][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (11/12) (9db6958e7070d1ddd8dc0ff744ee978b) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 618][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (9/12)#0 (d0b9957e0c5be6e650276ff20a50006e) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 618][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (10/12) (e191529b5423345b14e740a793696c8c) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 620][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 620][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (9/12)#0 (d0b9957e0c5be6e650276ff20a50006e) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 621][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (9/12) (d0b9957e0c5be6e650276ff20a50006e) switched from DEPLOYING to RUNNING.]
[WARN][2021-03-07 01:28:49 625][org.apache.flink.metrics.MetricGroup]-[The operator name Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) exceeded the 80 characters length limit and was truncated.]
[INFO][2021-03-07 01:28:49 625][com.alibaba.ververica.cdc.debezium.DebeziumSourceFunction]-[Consumer subtask 0 has no restore state.]
[INFO][2021-03-07 01:28:49 625][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 0 has no restore state.]
[INFO][2021-03-07 01:28:49 625][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (12/12)#0 (3be3b9e9654c94309f3c9da98452d53d), deploy into slot with allocation id 76b0a8587b580146382d96f4d2db67cd.]
[WARN][2021-03-07 01:28:49 626][org.apache.flink.metrics.MetricGroup]-[The operator name Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) exceeded the 80 characters length limit and was truncated.]
[INFO][2021-03-07 01:28:49 626][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 3 has no restore state.]
[INFO][2021-03-07 01:28:49 627][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 9 has no restore state.]
[INFO][2021-03-07 01:28:49 627][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 5 has no restore state.]
[INFO][2021-03-07 01:28:49 627][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 11 has no restore state.]
[WARN][2021-03-07 01:28:49 627][org.apache.flink.metrics.MetricGroup]-[The operator name Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) exceeded the 80 characters length limit and was truncated.]
[INFO][2021-03-07 01:28:49 625][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 1 has no restore state.]
[INFO][2021-03-07 01:28:49 628][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 7 has no restore state.]
[INFO][2021-03-07 01:28:49 628][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 6 has no restore state.]
[INFO][2021-03-07 01:28:49 628][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 2 has no restore state.]
[INFO][2021-03-07 01:28:49 629][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 10 has no restore state.]
[INFO][2021-03-07 01:28:49 628][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 4 has no restore state.]
[INFO][2021-03-07 01:28:49 628][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (12/12)#0 (3be3b9e9654c94309f3c9da98452d53d) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 629][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (12/12)#0 (3be3b9e9654c94309f3c9da98452d53d) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 627][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot a245cfd2a749df92a15769eca12db2be.]
[INFO][2021-03-07 01:28:49 631][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (12/12)#0 (3be3b9e9654c94309f3c9da98452d53d) [DEPLOYING].]
[WARN][2021-03-07 01:28:49 629][org.apache.flink.metrics.MetricGroup]-[The operator name Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) exceeded the 80 characters length limit and was truncated.]
[INFO][2021-03-07 01:28:49 628][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 8 has no restore state.]
[INFO][2021-03-07 01:28:49 632][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (1/12)#0 (3a2ad6257fb3a80bcb405ae55a722bf9), deploy into slot with allocation id a245cfd2a749df92a15769eca12db2be.]
[INFO][2021-03-07 01:28:49 633][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 25496b8033d98de9be27839608eee066.]
[INFO][2021-03-07 01:28:49 633][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (1/12)#0 (3a2ad6257fb3a80bcb405ae55a722bf9) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 633][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (1/12)#0 (3a2ad6257fb3a80bcb405ae55a722bf9) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 634][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 634][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (1/12)#0 (3a2ad6257fb3a80bcb405ae55a722bf9) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 634][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (12/12)#0 (3be3b9e9654c94309f3c9da98452d53d) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 634][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name]) (12/12) (3be3b9e9654c94309f3c9da98452d53d) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 635][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (2/12)#0 (c04c3ad746a573401a119e7890251457), deploy into slot with allocation id 25496b8033d98de9be27839608eee066.]
[INFO][2021-03-07 01:28:49 635][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 635][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (1/12)#0 (3a2ad6257fb3a80bcb405ae55a722bf9) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 635][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (2/12)#0 (c04c3ad746a573401a119e7890251457) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 635][com.alibaba.ververica.cdc.debezium.DebeziumSourceFunction]-[Debezium Properties:
	database.server.name = mysql_binlog_source
	value.converter.schemas.enable = false
	offset.flush.interval.ms = 9223372036854775807
	database.history.instance.name = 4b1dc2ea-c5e3-41a7-83f3-0f136e6dd761
	database.history = com.alibaba.ververica.cdc.debezium.internal.FlinkDatabaseHistory
	table.whitelist = traffic.testflink
	database.hostname = localhost
	offset.storage = com.alibaba.ververica.cdc.debezium.internal.FlinkOffsetBackingStore
	connector.class = io.debezium.connector.mysql.MySqlConnector
	database.history.skip.unparseable.ddl = true
	database.password = 12345678
	include.schema.changes = false
	name = engine
	database.port = 3306
	tombstones.on.delete = false
	key.converter.schemas.enable = false
	database.user = root
	database.whitelist = traffic
	database.serverTimezone = Asia/Shanghai
]
[INFO][2021-03-07 01:28:49 635][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 7b41d4e42a57905538a862dc6f41377f.]
[INFO][2021-03-07 01:28:49 635][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (1/12) (3a2ad6257fb3a80bcb405ae55a722bf9) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 636][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (2/12)#0 (c04c3ad746a573401a119e7890251457) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 637][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (2/12)#0 (c04c3ad746a573401a119e7890251457) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 637][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 637][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (2/12)#0 (c04c3ad746a573401a119e7890251457) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 638][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (2/12) (c04c3ad746a573401a119e7890251457) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 638][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (3/12)#0 (49993121b2bae42027be4a7a79e9f137), deploy into slot with allocation id 7b41d4e42a57905538a862dc6f41377f.]
[INFO][2021-03-07 01:28:49 638][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot f8689d61dda41c6ded47c656789b3051.]
[INFO][2021-03-07 01:28:49 638][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (3/12)#0 (49993121b2bae42027be4a7a79e9f137) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 639][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (3/12)#0 (49993121b2bae42027be4a7a79e9f137) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 639][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (3/12)#0 (49993121b2bae42027be4a7a79e9f137) [DEPLOYING].]
[WARN][2021-03-07 01:28:49 640][org.apache.flink.metrics.MetricGroup]-[The operator name Join(joinType=[InnerJoin], where=[(userId = id)], select=[userId, orderId, id, name], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) exceeded the 80 characters length limit and was truncated.]
[INFO][2021-03-07 01:28:49 641][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 641][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (3/12)#0 (49993121b2bae42027be4a7a79e9f137) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 641][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (3/12) (49993121b2bae42027be4a7a79e9f137) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 642][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (4/12)#0 (b99fac6c26d8d31cd214b70ede5f8408), deploy into slot with allocation id f8689d61dda41c6ded47c656789b3051.]
[INFO][2021-03-07 01:28:49 642][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (4/12)#0 (b99fac6c26d8d31cd214b70ede5f8408) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 642][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 3e792c09e4a4b1f17340028d3f492c0b.]
[INFO][2021-03-07 01:28:49 642][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (4/12)#0 (b99fac6c26d8d31cd214b70ede5f8408) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 644][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (4/12)#0 (b99fac6c26d8d31cd214b70ede5f8408) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 646][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (5/12)#0 (a972e3542a745aaface3f6884d3be146), deploy into slot with allocation id 3e792c09e4a4b1f17340028d3f492c0b.]
[INFO][2021-03-07 01:28:49 647][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 647][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (4/12)#0 (b99fac6c26d8d31cd214b70ede5f8408) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 647][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 385c2b39c682065ee1b01a0d53a04242.]
[INFO][2021-03-07 01:28:49 647][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (5/12)#0 (a972e3542a745aaface3f6884d3be146) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 647][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (4/12) (b99fac6c26d8d31cd214b70ede5f8408) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 647][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (5/12)#0 (a972e3542a745aaface3f6884d3be146) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 649][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (5/12)#0 (a972e3542a745aaface3f6884d3be146) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 650][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 650][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (5/12)#0 (a972e3542a745aaface3f6884d3be146) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 651][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (6/12)#0 (303003e33bea497b92f3c6b4ed1fa944), deploy into slot with allocation id 385c2b39c682065ee1b01a0d53a04242.]
[INFO][2021-03-07 01:28:49 652][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (5/12) (a972e3542a745aaface3f6884d3be146) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 653][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot b16595f537326ab9da08466aa47563d3.]
[INFO][2021-03-07 01:28:49 653][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (6/12)#0 (303003e33bea497b92f3c6b4ed1fa944) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 653][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (6/12)#0 (303003e33bea497b92f3c6b4ed1fa944) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 654][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (6/12)#0 (303003e33bea497b92f3c6b4ed1fa944) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 655][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 655][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (6/12)#0 (303003e33bea497b92f3c6b4ed1fa944) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 656][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (7/12)#0 (979d770095d912b2cf79c375665d892b), deploy into slot with allocation id b16595f537326ab9da08466aa47563d3.]
[INFO][2021-03-07 01:28:49 656][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (6/12) (303003e33bea497b92f3c6b4ed1fa944) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 656][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 3a91bdb43645961c87c8fd500ff54728.]
[INFO][2021-03-07 01:28:49 657][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (7/12)#0 (979d770095d912b2cf79c375665d892b) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 657][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (7/12)#0 (979d770095d912b2cf79c375665d892b) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 660][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (7/12)#0 (979d770095d912b2cf79c375665d892b) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 662][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 662][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (8/12)#0 (f8b66729777ee0b7da16ae8c8bca4842), deploy into slot with allocation id 3a91bdb43645961c87c8fd500ff54728.]
[INFO][2021-03-07 01:28:49 662][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (7/12)#0 (979d770095d912b2cf79c375665d892b) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 662][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (7/12) (979d770095d912b2cf79c375665d892b) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 663][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot ce26798dd40fb2d0655b066bd1a24372.]
[INFO][2021-03-07 01:28:49 663][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (8/12)#0 (f8b66729777ee0b7da16ae8c8bca4842) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 663][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (8/12)#0 (f8b66729777ee0b7da16ae8c8bca4842) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 663][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (8/12)#0 (f8b66729777ee0b7da16ae8c8bca4842) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 664][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 664][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (9/12)#0 (3adb7e604c13ae04b4bfe0b9a929e56f), deploy into slot with allocation id ce26798dd40fb2d0655b066bd1a24372.]
[INFO][2021-03-07 01:28:49 664][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (8/12)#0 (f8b66729777ee0b7da16ae8c8bca4842) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 665][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (8/12) (f8b66729777ee0b7da16ae8c8bca4842) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 665][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (9/12)#0 (3adb7e604c13ae04b4bfe0b9a929e56f) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 665][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot eacb317cb275b479e8fcbbb3ea7016a6.]
[INFO][2021-03-07 01:28:49 665][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (9/12)#0 (3adb7e604c13ae04b4bfe0b9a929e56f) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 666][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (9/12)#0 (3adb7e604c13ae04b4bfe0b9a929e56f) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 666][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 667][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (9/12)#0 (3adb7e604c13ae04b4bfe0b9a929e56f) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 667][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (9/12) (3adb7e604c13ae04b4bfe0b9a929e56f) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 668][org.apache.flink.runtime.state.heap.HeapKeyedStateBackend]-[Initializing heap keyed state backend with stream factory.]
[INFO][2021-03-07 01:28:49 668][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (10/12)#0 (2e7d0f61fa31884d84a63807a51288fd), deploy into slot with allocation id eacb317cb275b479e8fcbbb3ea7016a6.]
[INFO][2021-03-07 01:28:49 668][org.apache.flink.runtime.state.heap.HeapKeyedStateBackend]-[Initializing heap keyed state backend with stream factory.]
[INFO][2021-03-07 01:28:49 668][org.apache.flink.runtime.state.heap.HeapKeyedStateBackend]-[Initializing heap keyed state backend with stream factory.]
[INFO][2021-03-07 01:28:49 668][org.apache.flink.runtime.state.heap.HeapKeyedStateBackend]-[Initializing heap keyed state backend with stream factory.]
[INFO][2021-03-07 01:28:49 668][org.apache.flink.runtime.state.heap.HeapKeyedStateBackend]-[Initializing heap keyed state backend with stream factory.]
[INFO][2021-03-07 01:28:49 668][org.apache.flink.runtime.state.heap.HeapKeyedStateBackend]-[Initializing heap keyed state backend with stream factory.]
[INFO][2021-03-07 01:28:49 668][org.apache.flink.runtime.state.heap.HeapKeyedStateBackend]-[Initializing heap keyed state backend with stream factory.]
[INFO][2021-03-07 01:28:49 668][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 09e3a764b09fbbbc3e7b5d94131ca3cd.]
[INFO][2021-03-07 01:28:49 668][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (10/12)#0 (2e7d0f61fa31884d84a63807a51288fd) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 668][org.apache.flink.runtime.state.heap.HeapKeyedStateBackend]-[Initializing heap keyed state backend with stream factory.]
[INFO][2021-03-07 01:28:49 668][org.apache.flink.runtime.state.heap.HeapKeyedStateBackend]-[Initializing heap keyed state backend with stream factory.]
[INFO][2021-03-07 01:28:49 668][org.apache.flink.runtime.state.heap.HeapKeyedStateBackend]-[Initializing heap keyed state backend with stream factory.]
[INFO][2021-03-07 01:28:49 668][org.apache.flink.runtime.state.heap.HeapKeyedStateBackend]-[Initializing heap keyed state backend with stream factory.]
[INFO][2021-03-07 01:28:49 668][org.apache.flink.runtime.state.heap.HeapKeyedStateBackend]-[Initializing heap keyed state backend with stream factory.]
[INFO][2021-03-07 01:28:49 668][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (10/12)#0 (2e7d0f61fa31884d84a63807a51288fd) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 670][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (10/12)#0 (2e7d0f61fa31884d84a63807a51288fd) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 671][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 671][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (10/12)#0 (2e7d0f61fa31884d84a63807a51288fd) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 671][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (10/12) (2e7d0f61fa31884d84a63807a51288fd) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 673][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (11/12)#0 (b8a21e1f20db61e56e6d176422664e9f), deploy into slot with allocation id 09e3a764b09fbbbc3e7b5d94131ca3cd.]
[INFO][2021-03-07 01:28:49 674][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (11/12)#0 (b8a21e1f20db61e56e6d176422664e9f) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 674][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 76b0a8587b580146382d96f4d2db67cd.]
[INFO][2021-03-07 01:28:49 674][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (11/12)#0 (b8a21e1f20db61e56e6d176422664e9f) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 674][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (11/12)#0 (b8a21e1f20db61e56e6d176422664e9f) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 675][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 675][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (11/12)#0 (b8a21e1f20db61e56e6d176422664e9f) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 675][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (11/12) (b8a21e1f20db61e56e6d176422664e9f) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 676][org.apache.flink.runtime.taskexecutor.TaskExecutor]-[Received task Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (12/12)#0 (32f2fb6525d5156158194b6f80112bb4), deploy into slot with allocation id 76b0a8587b580146382d96f4d2db67cd.]
[INFO][2021-03-07 01:28:49 676][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (12/12)#0 (32f2fb6525d5156158194b6f80112bb4) switched from CREATED to DEPLOYING.]
[INFO][2021-03-07 01:28:49 676][org.apache.flink.runtime.taskmanager.Task]-[Loading JAR files for task Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (12/12)#0 (32f2fb6525d5156158194b6f80112bb4) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 676][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 25496b8033d98de9be27839608eee066.]
[INFO][2021-03-07 01:28:49 676][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot eacb317cb275b479e8fcbbb3ea7016a6.]
[INFO][2021-03-07 01:28:49 676][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot a245cfd2a749df92a15769eca12db2be.]
[INFO][2021-03-07 01:28:49 676][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot ce26798dd40fb2d0655b066bd1a24372.]
[INFO][2021-03-07 01:28:49 676][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 385c2b39c682065ee1b01a0d53a04242.]
[INFO][2021-03-07 01:28:49 676][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 76b0a8587b580146382d96f4d2db67cd.]
[INFO][2021-03-07 01:28:49 676][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 3a91bdb43645961c87c8fd500ff54728.]
[INFO][2021-03-07 01:28:49 677][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 3e792c09e4a4b1f17340028d3f492c0b.]
[INFO][2021-03-07 01:28:49 677][org.apache.flink.runtime.taskmanager.Task]-[Registering task at network: Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (12/12)#0 (32f2fb6525d5156158194b6f80112bb4) [DEPLOYING].]
[INFO][2021-03-07 01:28:49 677][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 09e3a764b09fbbbc3e7b5d94131ca3cd.]
[INFO][2021-03-07 01:28:49 677][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot f8689d61dda41c6ded47c656789b3051.]
[INFO][2021-03-07 01:28:49 677][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot 7b41d4e42a57905538a862dc6f41377f.]
[INFO][2021-03-07 01:28:49 677][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]-[Activate slot b16595f537326ab9da08466aa47563d3.]
[INFO][2021-03-07 01:28:49 678][org.apache.flink.streaming.runtime.tasks.StreamTask]-[No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)]
[INFO][2021-03-07 01:28:49 678][org.apache.flink.runtime.taskmanager.Task]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (12/12)#0 (32f2fb6525d5156158194b6f80112bb4) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 678][org.apache.flink.runtime.executiongraph.ExecutionGraph]-[Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) -> Calc(select=[userId, orderId, name, orderName]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (12/12) (32f2fb6525d5156158194b6f80112bb4) switched from DEPLOYING to RUNNING.]
[INFO][2021-03-07 01:28:49 681][org.apache.kafka.clients.consumer.ConsumerConfig]-[ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [node05:9092, node03:9092, node04:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
]
[INFO][2021-03-07 01:28:49 681][org.apache.kafka.clients.consumer.ConsumerConfig]-[ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [node05:9092, node03:9092, node04:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
]
[INFO][2021-03-07 01:28:49 681][org.apache.kafka.clients.consumer.ConsumerConfig]-[ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [node05:9092, node03:9092, node04:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
]
[INFO][2021-03-07 01:28:49 681][org.apache.kafka.clients.consumer.ConsumerConfig]-[ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [node05:9092, node03:9092, node04:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
]
[INFO][2021-03-07 01:28:49 681][org.apache.kafka.clients.consumer.ConsumerConfig]-[ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [node05:9092, node03:9092, node04:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
]
[INFO][2021-03-07 01:28:49 681][org.apache.kafka.clients.consumer.ConsumerConfig]-[ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [node05:9092, node03:9092, node04:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
]
[INFO][2021-03-07 01:28:49 683][org.apache.kafka.clients.consumer.ConsumerConfig]-[ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [node05:9092, node03:9092, node04:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
]
[INFO][2021-03-07 01:28:49 681][org.apache.kafka.clients.consumer.ConsumerConfig]-[ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [node05:9092, node03:9092, node04:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
]
[INFO][2021-03-07 01:28:49 681][org.apache.kafka.clients.consumer.ConsumerConfig]-[ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [node05:9092, node03:9092, node04:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
]
[INFO][2021-03-07 01:28:49 681][org.apache.kafka.clients.consumer.ConsumerConfig]-[ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [node05:9092, node03:9092, node04:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
]
[INFO][2021-03-07 01:28:49 683][org.apache.kafka.clients.consumer.ConsumerConfig]-[ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [node05:9092, node03:9092, node04:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
]
[INFO][2021-03-07 01:28:49 683][org.apache.kafka.clients.consumer.ConsumerConfig]-[ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [node05:9092, node03:9092, node04:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
]
[WARN][2021-03-07 01:28:49 692][org.apache.flink.metrics.MetricGroup]-[The operator name Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 693][org.apache.flink.metrics.MetricGroup]-[The operator name Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 694][org.apache.flink.metrics.MetricGroup]-[The operator name Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 694][org.apache.flink.metrics.MetricGroup]-[The operator name Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 694][org.apache.flink.metrics.MetricGroup]-[The operator name Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 694][org.apache.flink.metrics.MetricGroup]-[The operator name Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 695][org.apache.flink.metrics.MetricGroup]-[The operator name Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 695][org.apache.flink.metrics.MetricGroup]-[The operator name Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 696][org.apache.flink.metrics.MetricGroup]-[The operator name Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 696][org.apache.flink.metrics.MetricGroup]-[The operator name Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 696][org.apache.flink.metrics.MetricGroup]-[The operator name Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) exceeded the 80 characters length limit and was truncated.]
[WARN][2021-03-07 01:28:49 697][org.apache.flink.metrics.MetricGroup]-[The operator name Join(joinType=[InnerJoin], where=[(orderId = id)], select=[userId, orderId, name, orderName, id], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) exceeded the 80 characters length limit and was truncated.]
[INFO][2021-03-07 01:28:49 699][org.apache.flink.runtime.state.heap.HeapKeyedStateBackend]-[Initializing heap keyed state backend with stream factory.]
[INFO][2021-03-07 01:28:49 700][org.apache.flink.runtime.state.heap.HeapKeyedStateBackend]-[Initializing heap keyed state backend with stream factory.]
[INFO][2021-03-07 01:28:49 700][org.apache.flink.runtime.state.heap.HeapKeyedStateBackend]-[Initializing heap keyed state backend with stream factory.]
[INFO][2021-03-07 01:28:49 700][org.apache.flink.runtime.state.heap.HeapKeyedStateBackend]-[Initializing heap keyed state backend with stream factory.]
[INFO][2021-03-07 01:28:49 702][org.apache.flink.runtime.state.heap.HeapKeyedStateBackend]-[Initializing heap keyed state backend with stream factory.]
[INFO][2021-03-07 01:28:49 702][org.apache.flink.runtime.state.heap.HeapKeyedStateBackend]-[Initializing heap keyed state backend with stream factory.]
[INFO][2021-03-07 01:28:49 702][org.apache.flink.runtime.state.heap.HeapKeyedStateBackend]-[Initializing heap keyed state backend with stream factory.]
[INFO][2021-03-07 01:28:49 702][org.apache.flink.runtime.state.heap.HeapKeyedStateBackend]-[Initializing heap keyed state backend with stream factory.]
[INFO][2021-03-07 01:28:49 702][org.apache.flink.runtime.state.heap.HeapKeyedStateBackend]-[Initializing heap keyed state backend with stream factory.]
[INFO][2021-03-07 01:28:49 709][org.apache.flink.runtime.state.heap.HeapKeyedStateBackend]-[Initializing heap keyed state backend with stream factory.]
[INFO][2021-03-07 01:28:49 710][org.apache.flink.runtime.state.heap.HeapKeyedStateBackend]-[Initializing heap keyed state backend with stream factory.]
[INFO][2021-03-07 01:28:49 713][org.apache.flink.runtime.state.heap.HeapKeyedStateBackend]-[Initializing heap keyed state backend with stream factory.]
[WARN][2021-03-07 01:28:49 837][org.apache.kafka.clients.consumer.ConsumerConfig]-[The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.]
[WARN][2021-03-07 01:28:49 837][org.apache.kafka.clients.consumer.ConsumerConfig]-[The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.]
[WARN][2021-03-07 01:28:49 837][org.apache.kafka.clients.consumer.ConsumerConfig]-[The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.]
[WARN][2021-03-07 01:28:49 837][org.apache.kafka.clients.consumer.ConsumerConfig]-[The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.]
[WARN][2021-03-07 01:28:49 837][org.apache.kafka.clients.consumer.ConsumerConfig]-[The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.]
[WARN][2021-03-07 01:28:49 837][org.apache.kafka.clients.consumer.ConsumerConfig]-[The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.]
[WARN][2021-03-07 01:28:49 837][org.apache.kafka.clients.consumer.ConsumerConfig]-[The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.]
[WARN][2021-03-07 01:28:49 837][org.apache.kafka.clients.consumer.ConsumerConfig]-[The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.]
[WARN][2021-03-07 01:28:49 837][org.apache.kafka.clients.consumer.ConsumerConfig]-[The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.]
[WARN][2021-03-07 01:28:49 837][org.apache.kafka.clients.consumer.ConsumerConfig]-[The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.]
[WARN][2021-03-07 01:28:49 838][org.apache.kafka.clients.consumer.ConsumerConfig]-[The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.]
[WARN][2021-03-07 01:28:49 838][org.apache.kafka.clients.consumer.ConsumerConfig]-[The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.]
[INFO][2021-03-07 01:28:49 841][org.apache.kafka.common.utils.AppInfoParser]-[Kafka version: 2.4.1]
[INFO][2021-03-07 01:28:49 841][org.apache.kafka.common.utils.AppInfoParser]-[Kafka commitId: c57222ae8cd7866b]
[INFO][2021-03-07 01:28:49 841][org.apache.kafka.common.utils.AppInfoParser]-[Kafka startTimeMs: 1615051729838]
[INFO][2021-03-07 01:28:49 843][org.apache.kafka.common.utils.AppInfoParser]-[Kafka version: 2.4.1]
[INFO][2021-03-07 01:28:49 843][org.apache.kafka.common.utils.AppInfoParser]-[Kafka commitId: c57222ae8cd7866b]
[INFO][2021-03-07 01:28:49 843][org.apache.kafka.common.utils.AppInfoParser]-[Kafka startTimeMs: 1615051729837]
[INFO][2021-03-07 01:28:49 844][org.apache.kafka.common.utils.AppInfoParser]-[Kafka version: 2.4.1]
[INFO][2021-03-07 01:28:49 844][org.apache.kafka.common.utils.AppInfoParser]-[Kafka commitId: c57222ae8cd7866b]
[INFO][2021-03-07 01:28:49 844][org.apache.kafka.common.utils.AppInfoParser]-[Kafka startTimeMs: 1615051729837]
[INFO][2021-03-07 01:28:49 844][org.apache.kafka.common.utils.AppInfoParser]-[Kafka version: 2.4.1]
[INFO][2021-03-07 01:28:49 844][org.apache.kafka.common.utils.AppInfoParser]-[Kafka commitId: c57222ae8cd7866b]
[INFO][2021-03-07 01:28:49 844][org.apache.kafka.common.utils.AppInfoParser]-[Kafka startTimeMs: 1615051729837]
[INFO][2021-03-07 01:28:49 844][org.apache.kafka.common.utils.AppInfoParser]-[Kafka version: 2.4.1]
[INFO][2021-03-07 01:28:49 844][org.apache.kafka.common.utils.AppInfoParser]-[Kafka commitId: c57222ae8cd7866b]
[INFO][2021-03-07 01:28:49 844][org.apache.kafka.common.utils.AppInfoParser]-[Kafka startTimeMs: 1615051729837]
[INFO][2021-03-07 01:28:49 844][org.apache.kafka.common.utils.AppInfoParser]-[Kafka version: 2.4.1]
[INFO][2021-03-07 01:28:49 844][org.apache.kafka.common.utils.AppInfoParser]-[Kafka commitId: c57222ae8cd7866b]
[INFO][2021-03-07 01:28:49 844][org.apache.kafka.common.utils.AppInfoParser]-[Kafka startTimeMs: 1615051729838]
[INFO][2021-03-07 01:28:49 845][org.apache.kafka.common.utils.AppInfoParser]-[Kafka version: 2.4.1]
[INFO][2021-03-07 01:28:49 845][org.apache.kafka.common.utils.AppInfoParser]-[Kafka commitId: c57222ae8cd7866b]
[INFO][2021-03-07 01:28:49 845][org.apache.kafka.common.utils.AppInfoParser]-[Kafka startTimeMs: 1615051729837]
[INFO][2021-03-07 01:28:49 845][org.apache.kafka.common.utils.AppInfoParser]-[Kafka version: 2.4.1]
[INFO][2021-03-07 01:28:49 845][org.apache.kafka.common.utils.AppInfoParser]-[Kafka commitId: c57222ae8cd7866b]
[INFO][2021-03-07 01:28:49 845][org.apache.kafka.common.utils.AppInfoParser]-[Kafka startTimeMs: 1615051729837]
[INFO][2021-03-07 01:28:49 845][org.apache.kafka.common.utils.AppInfoParser]-[Kafka version: 2.4.1]
[INFO][2021-03-07 01:28:49 845][org.apache.kafka.common.utils.AppInfoParser]-[Kafka commitId: c57222ae8cd7866b]
[INFO][2021-03-07 01:28:49 845][org.apache.kafka.common.utils.AppInfoParser]-[Kafka startTimeMs: 1615051729837]
[INFO][2021-03-07 01:28:49 845][org.apache.kafka.common.utils.AppInfoParser]-[Kafka version: 2.4.1]
[INFO][2021-03-07 01:28:49 845][org.apache.kafka.common.utils.AppInfoParser]-[Kafka commitId: c57222ae8cd7866b]
[INFO][2021-03-07 01:28:49 845][org.apache.kafka.common.utils.AppInfoParser]-[Kafka startTimeMs: 1615051729837]
[INFO][2021-03-07 01:28:49 845][org.apache.kafka.common.utils.AppInfoParser]-[Kafka version: 2.4.1]
[INFO][2021-03-07 01:28:49 845][org.apache.kafka.common.utils.AppInfoParser]-[Kafka commitId: c57222ae8cd7866b]
[INFO][2021-03-07 01:28:49 846][org.apache.kafka.common.utils.AppInfoParser]-[Kafka startTimeMs: 1615051729838]
[INFO][2021-03-07 01:28:49 846][org.apache.kafka.common.utils.AppInfoParser]-[Kafka version: 2.4.1]
[INFO][2021-03-07 01:28:49 846][org.apache.kafka.common.utils.AppInfoParser]-[Kafka commitId: c57222ae8cd7866b]
[INFO][2021-03-07 01:28:49 846][org.apache.kafka.common.utils.AppInfoParser]-[Kafka startTimeMs: 1615051729837]
[INFO][2021-03-07 01:28:49 935][org.apache.kafka.connect.json.JsonConverterConfig]-[JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
]
[INFO][2021-03-07 01:28:49 939][org.apache.kafka.connect.json.JsonConverterConfig]-[JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
]
[INFO][2021-03-07 01:28:49 942][io.debezium.embedded.EmbeddedEngine$EmbeddedConfig]-[EmbeddedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = default
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 9223372036854775807
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = 
	offset.storage.partitions = null
	offset.storage.replication.factor = null
	offset.storage.topic = 
	plugin.path = null
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.client.auth = none
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
]
[INFO][2021-03-07 01:28:49 942][org.apache.kafka.connect.runtime.WorkerConfig]-[Worker configuration property 'internal.key.converter' is deprecated and may be removed in an upcoming release. The specified value 'org.apache.kafka.connect.json.JsonConverter' matches the default, so this property can be safely removed from the worker configuration.]
[INFO][2021-03-07 01:28:49 943][org.apache.kafka.connect.runtime.WorkerConfig]-[Worker configuration property 'internal.value.converter' is deprecated and may be removed in an upcoming release. The specified value 'org.apache.kafka.connect.json.JsonConverter' matches the default, so this property can be safely removed from the worker configuration.]
[INFO][2021-03-07 01:28:49 998][io.debezium.connector.common.BaseSourceTask]-[Starting MySqlConnectorTask with configuration:]
[INFO][2021-03-07 01:28:50 000][io.debezium.connector.common.BaseSourceTask]-[   connector.class = io.debezium.connector.mysql.MySqlConnector]
[INFO][2021-03-07 01:28:50 000][io.debezium.connector.common.BaseSourceTask]-[   database.user = root]
[INFO][2021-03-07 01:28:50 000][io.debezium.connector.common.BaseSourceTask]-[   offset.storage = com.alibaba.ververica.cdc.debezium.internal.FlinkOffsetBackingStore]
[INFO][2021-03-07 01:28:50 000][io.debezium.connector.common.BaseSourceTask]-[   database.server.name = mysql_binlog_source]
[INFO][2021-03-07 01:28:50 000][io.debezium.connector.common.BaseSourceTask]-[   include.schema.changes = false]
[INFO][2021-03-07 01:28:50 000][io.debezium.connector.common.BaseSourceTask]-[   database.port = 3306]
[INFO][2021-03-07 01:28:50 000][io.debezium.connector.common.BaseSourceTask]-[   table.whitelist = traffic.testflink]
[INFO][2021-03-07 01:28:50 000][io.debezium.connector.common.BaseSourceTask]-[   offset.flush.interval.ms = 9223372036854775807]
[INFO][2021-03-07 01:28:50 000][io.debezium.connector.common.BaseSourceTask]-[   key.converter.schemas.enable = false]
[INFO][2021-03-07 01:28:50 000][io.debezium.connector.common.BaseSourceTask]-[   tombstones.on.delete = false]
[INFO][2021-03-07 01:28:50 000][io.debezium.connector.common.BaseSourceTask]-[   database.serverTimezone = Asia/Shanghai]
[INFO][2021-03-07 01:28:50 000][io.debezium.connector.common.BaseSourceTask]-[   database.hostname = localhost]
[INFO][2021-03-07 01:28:50 000][io.debezium.connector.common.BaseSourceTask]-[   database.password = ********]
[INFO][2021-03-07 01:28:50 000][io.debezium.connector.common.BaseSourceTask]-[   value.converter.schemas.enable = false]
[INFO][2021-03-07 01:28:50 000][io.debezium.connector.common.BaseSourceTask]-[   name = engine]
[INFO][2021-03-07 01:28:50 000][io.debezium.connector.common.BaseSourceTask]-[   database.history.skip.unparseable.ddl = true]
[INFO][2021-03-07 01:28:50 000][io.debezium.connector.common.BaseSourceTask]-[   database.whitelist = traffic]
[INFO][2021-03-07 01:28:50 000][io.debezium.connector.common.BaseSourceTask]-[   database.history.instance.name = 4b1dc2ea-c5e3-41a7-83f3-0f136e6dd761]
[INFO][2021-03-07 01:28:50 000][io.debezium.connector.common.BaseSourceTask]-[   database.history = com.alibaba.ververica.cdc.debezium.internal.FlinkDatabaseHistory]
[INFO][2021-03-07 01:28:50 215][org.apache.kafka.clients.Metadata]-[[Consumer clientId=consumer-flink4-9, groupId=flink4] Cluster ID: p5zizhkESiOh_qQkrl3oJg]
[INFO][2021-03-07 01:28:50 215][org.apache.kafka.clients.Metadata]-[[Consumer clientId=consumer-flink4-6, groupId=flink4] Cluster ID: p5zizhkESiOh_qQkrl3oJg]
[INFO][2021-03-07 01:28:50 215][org.apache.kafka.clients.Metadata]-[[Consumer clientId=consumer-flink4-1, groupId=flink4] Cluster ID: p5zizhkESiOh_qQkrl3oJg]
[INFO][2021-03-07 01:28:50 215][org.apache.kafka.clients.Metadata]-[[Consumer clientId=consumer-flink4-5, groupId=flink4] Cluster ID: p5zizhkESiOh_qQkrl3oJg]
[INFO][2021-03-07 01:28:50 215][org.apache.kafka.clients.Metadata]-[[Consumer clientId=consumer-flink4-7, groupId=flink4] Cluster ID: p5zizhkESiOh_qQkrl3oJg]
[INFO][2021-03-07 01:28:50 215][org.apache.kafka.clients.Metadata]-[[Consumer clientId=consumer-flink4-10, groupId=flink4] Cluster ID: p5zizhkESiOh_qQkrl3oJg]
[INFO][2021-03-07 01:28:50 215][org.apache.kafka.clients.Metadata]-[[Consumer clientId=consumer-flink4-12, groupId=flink4] Cluster ID: p5zizhkESiOh_qQkrl3oJg]
[INFO][2021-03-07 01:28:50 215][org.apache.kafka.clients.Metadata]-[[Consumer clientId=consumer-flink4-3, groupId=flink4] Cluster ID: p5zizhkESiOh_qQkrl3oJg]
[INFO][2021-03-07 01:28:50 215][org.apache.kafka.clients.Metadata]-[[Consumer clientId=consumer-flink4-8, groupId=flink4] Cluster ID: p5zizhkESiOh_qQkrl3oJg]
[INFO][2021-03-07 01:28:50 215][org.apache.kafka.clients.Metadata]-[[Consumer clientId=consumer-flink4-4, groupId=flink4] Cluster ID: p5zizhkESiOh_qQkrl3oJg]
[INFO][2021-03-07 01:28:50 215][org.apache.kafka.clients.Metadata]-[[Consumer clientId=consumer-flink4-2, groupId=flink4] Cluster ID: p5zizhkESiOh_qQkrl3oJg]
[INFO][2021-03-07 01:28:50 215][org.apache.kafka.clients.Metadata]-[[Consumer clientId=consumer-flink4-11, groupId=flink4] Cluster ID: p5zizhkESiOh_qQkrl3oJg]
[INFO][2021-03-07 01:28:50 219][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 0 initially has no partitions to read from.]
[INFO][2021-03-07 01:28:50 219][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 10 will start reading the following 1 partitions from the earliest offsets: [KafkaTopicPartition{topic='flink2', partition=0}]]
[INFO][2021-03-07 01:28:50 219][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 11 will start reading the following 1 partitions from the earliest offsets: [KafkaTopicPartition{topic='flink2', partition=1}]]
[INFO][2021-03-07 01:28:50 219][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 9 initially has no partitions to read from.]
[INFO][2021-03-07 01:28:50 219][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 4 initially has no partitions to read from.]
[INFO][2021-03-07 01:28:50 219][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 3 initially has no partitions to read from.]
[INFO][2021-03-07 01:28:50 219][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 5 initially has no partitions to read from.]
[INFO][2021-03-07 01:28:50 219][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 2 initially has no partitions to read from.]
[INFO][2021-03-07 01:28:50 219][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 6 initially has no partitions to read from.]
[INFO][2021-03-07 01:28:50 219][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 8 initially has no partitions to read from.]
[INFO][2021-03-07 01:28:50 219][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 7 initially has no partitions to read from.]
[INFO][2021-03-07 01:28:50 219][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 1 initially has no partitions to read from.]
[INFO][2021-03-07 01:28:50 225][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 10 creating fetcher with offsets {KafkaTopicPartition{topic='flink2', partition=0}=-915623761775}.]
[INFO][2021-03-07 01:28:50 225][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 11 creating fetcher with offsets {KafkaTopicPartition{topic='flink2', partition=1}=-915623761775}.]
[INFO][2021-03-07 01:28:50 228][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 2 creating fetcher with offsets {}.]
[INFO][2021-03-07 01:28:50 228][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 9 creating fetcher with offsets {}.]
[INFO][2021-03-07 01:28:50 228][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 5 creating fetcher with offsets {}.]
[INFO][2021-03-07 01:28:50 228][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 6 creating fetcher with offsets {}.]
[INFO][2021-03-07 01:28:50 228][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 8 creating fetcher with offsets {}.]
[INFO][2021-03-07 01:28:50 228][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 7 creating fetcher with offsets {}.]
[INFO][2021-03-07 01:28:50 228][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 3 creating fetcher with offsets {}.]
[INFO][2021-03-07 01:28:50 228][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 1 creating fetcher with offsets {}.]
[INFO][2021-03-07 01:28:50 228][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 0 creating fetcher with offsets {}.]
[INFO][2021-03-07 01:28:50 228][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]-[Consumer subtask 4 creating fetcher with offsets {}.]
[INFO][2021-03-07 01:28:50 342][org.apache.kafka.clients.consumer.ConsumerConfig]-[ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [node05:9092, node03:9092, node04:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
]
[INFO][2021-03-07 01:28:50 342][org.apache.kafka.clients.consumer.ConsumerConfig]-[ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [node05:9092, node03:9092, node04:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
]
[INFO][2021-03-07 01:28:50 342][org.apache.kafka.clients.consumer.ConsumerConfig]-[ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [node05:9092, node03:9092, node04:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
]
[INFO][2021-03-07 01:28:50 342][org.apache.kafka.clients.consumer.ConsumerConfig]-[ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [node05:9092, node03:9092, node04:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
]
[INFO][2021-03-07 01:28:50 344][org.apache.kafka.clients.consumer.ConsumerConfig]-[ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [node05:9092, node03:9092, node04:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
]
[INFO][2021-03-07 01:28:50 343][org.apache.kafka.clients.consumer.ConsumerConfig]-[ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [node05:9092, node03:9092, node04:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
]
[INFO][2021-03-07 01:28:50 342][org.apache.kafka.clients.consumer.ConsumerConfig]-[ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [node05:9092, node03:9092, node04:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
]
[INFO][2021-03-07 01:28:50 345][org.apache.kafka.clients.consumer.ConsumerConfig]-[ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [node05:9092, node03:9092, node04:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
]
[INFO][2021-03-07 01:28:50 345][org.apache.kafka.clients.consumer.ConsumerConfig]-[ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [node05:9092, node03:9092, node04:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
]
[INFO][2021-03-07 01:28:50 347][org.apache.kafka.clients.consumer.ConsumerConfig]-[ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [node05:9092, node03:9092, node04:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
]
[INFO][2021-03-07 01:28:50 357][org.apache.kafka.clients.consumer.ConsumerConfig]-[ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [node05:9092, node03:9092, node04:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
]
[WARN][2021-03-07 01:28:50 357][org.apache.kafka.clients.consumer.ConsumerConfig]-[The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.]
[INFO][2021-03-07 01:28:50 357][org.apache.kafka.clients.consumer.ConsumerConfig]-[ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [node05:9092, node03:9092, node04:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
]
[INFO][2021-03-07 01:28:50 359][org.apache.kafka.common.utils.AppInfoParser]-[Kafka version: 2.4.1]
[INFO][2021-03-07 01:28:50 359][org.apache.kafka.common.utils.AppInfoParser]-[Kafka commitId: c57222ae8cd7866b]
[INFO][2021-03-07 01:28:50 359][org.apache.kafka.common.utils.AppInfoParser]-[Kafka startTimeMs: 1615051730359]
[WARN][2021-03-07 01:28:50 362][org.apache.kafka.clients.consumer.ConsumerConfig]-[The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.]
[INFO][2021-03-07 01:28:50 362][org.apache.kafka.common.utils.AppInfoParser]-[Kafka version: 2.4.1]
[INFO][2021-03-07 01:28:50 362][org.apache.kafka.common.utils.AppInfoParser]-[Kafka commitId: c57222ae8cd7866b]
[INFO][2021-03-07 01:28:50 362][org.apache.kafka.common.utils.AppInfoParser]-[Kafka startTimeMs: 1615051730362]
[WARN][2021-03-07 01:28:50 362][org.apache.kafka.clients.consumer.ConsumerConfig]-[The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.]
[INFO][2021-03-07 01:28:50 362][org.apache.kafka.common.utils.AppInfoParser]-[Kafka version: 2.4.1]
[INFO][2021-03-07 01:28:50 363][org.apache.kafka.common.utils.AppInfoParser]-[Kafka commitId: c57222ae8cd7866b]
[INFO][2021-03-07 01:28:50 363][org.apache.kafka.common.utils.AppInfoParser]-[Kafka startTimeMs: 1615051730362]
[WARN][2021-03-07 01:28:50 364][org.apache.kafka.clients.consumer.ConsumerConfig]-[The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.]
[WARN][2021-03-07 01:28:50 364][org.apache.kafka.clients.consumer.ConsumerConfig]-[The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.]
[INFO][2021-03-07 01:28:50 365][org.apache.kafka.common.utils.AppInfoParser]-[Kafka version: 2.4.1]
[INFO][2021-03-07 01:28:50 365][org.apache.kafka.common.utils.AppInfoParser]-[Kafka commitId: c57222ae8cd7866b]
[INFO][2021-03-07 01:28:50 365][org.apache.kafka.common.utils.AppInfoParser]-[Kafka startTimeMs: 1615051730364]
[INFO][2021-03-07 01:28:50 366][org.apache.kafka.common.utils.AppInfoParser]-[Kafka version: 2.4.1]
[INFO][2021-03-07 01:28:50 366][org.apache.kafka.common.utils.AppInfoParser]-[Kafka commitId: c57222ae8cd7866b]
[INFO][2021-03-07 01:28:50 366][org.apache.kafka.common.utils.AppInfoParser]-[Kafka startTimeMs: 1615051730364]
[WARN][2021-03-07 01:28:50 366][org.apache.kafka.clients.consumer.ConsumerConfig]-[The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.]
[WARN][2021-03-07 01:28:50 367][org.apache.kafka.clients.consumer.ConsumerConfig]-[The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.]
[INFO][2021-03-07 01:28:50 367][org.apache.kafka.common.utils.AppInfoParser]-[Kafka version: 2.4.1]
[INFO][2021-03-07 01:28:50 367][org.apache.kafka.common.utils.AppInfoParser]-[Kafka commitId: c57222ae8cd7866b]
[INFO][2021-03-07 01:28:50 367][org.apache.kafka.common.utils.AppInfoParser]-[Kafka startTimeMs: 1615051730367]
[WARN][2021-03-07 01:28:50 368][org.apache.kafka.clients.consumer.ConsumerConfig]-[The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.]
[INFO][2021-03-07 01:28:50 368][org.apache.kafka.common.utils.AppInfoParser]-[Kafka version: 2.4.1]
[INFO][2021-03-07 01:28:50 368][org.apache.kafka.common.utils.AppInfoParser]-[Kafka commitId: c57222ae8cd7866b]
[INFO][2021-03-07 01:28:50 368][org.apache.kafka.common.utils.AppInfoParser]-[Kafka startTimeMs: 1615051730367]
[INFO][2021-03-07 01:28:50 368][org.apache.kafka.common.utils.AppInfoParser]-[Kafka version: 2.4.1]
[INFO][2021-03-07 01:28:50 369][org.apache.kafka.common.utils.AppInfoParser]-[Kafka commitId: c57222ae8cd7866b]
[INFO][2021-03-07 01:28:50 369][org.apache.kafka.common.utils.AppInfoParser]-[Kafka startTimeMs: 1615051730368]
[WARN][2021-03-07 01:28:50 370][org.apache.kafka.clients.consumer.ConsumerConfig]-[The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.]
[INFO][2021-03-07 01:28:50 370][org.apache.kafka.common.utils.AppInfoParser]-[Kafka version: 2.4.1]
[INFO][2021-03-07 01:28:50 370][org.apache.kafka.common.utils.AppInfoParser]-[Kafka commitId: c57222ae8cd7866b]
[INFO][2021-03-07 01:28:50 370][org.apache.kafka.common.utils.AppInfoParser]-[Kafka startTimeMs: 1615051730370]
[WARN][2021-03-07 01:28:50 370][org.apache.kafka.clients.consumer.ConsumerConfig]-[The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.]
[INFO][2021-03-07 01:28:50 371][org.apache.kafka.common.utils.AppInfoParser]-[Kafka version: 2.4.1]
[INFO][2021-03-07 01:28:50 371][org.apache.kafka.common.utils.AppInfoParser]-[Kafka commitId: c57222ae8cd7866b]
[INFO][2021-03-07 01:28:50 371][org.apache.kafka.common.utils.AppInfoParser]-[Kafka startTimeMs: 1615051730371]
[WARN][2021-03-07 01:28:50 372][org.apache.kafka.clients.consumer.ConsumerConfig]-[The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.]
[INFO][2021-03-07 01:28:50 372][org.apache.kafka.common.utils.AppInfoParser]-[Kafka version: 2.4.1]
[INFO][2021-03-07 01:28:50 372][org.apache.kafka.common.utils.AppInfoParser]-[Kafka commitId: c57222ae8cd7866b]
[INFO][2021-03-07 01:28:50 372][org.apache.kafka.common.utils.AppInfoParser]-[Kafka startTimeMs: 1615051730372]
[WARN][2021-03-07 01:28:50 372][org.apache.kafka.clients.consumer.ConsumerConfig]-[The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.]
[INFO][2021-03-07 01:28:50 372][org.apache.kafka.common.utils.AppInfoParser]-[Kafka version: 2.4.1]
[INFO][2021-03-07 01:28:50 372][org.apache.kafka.common.utils.AppInfoParser]-[Kafka commitId: c57222ae8cd7866b]
[INFO][2021-03-07 01:28:50 373][org.apache.kafka.common.utils.AppInfoParser]-[Kafka startTimeMs: 1615051730372]
[INFO][2021-03-07 01:28:50 378][org.apache.kafka.clients.consumer.KafkaConsumer]-[[Consumer clientId=consumer-flink4-23, groupId=flink4] Subscribed to partition(s): flink2-1]
[INFO][2021-03-07 01:28:50 379][org.apache.kafka.clients.consumer.KafkaConsumer]-[[Consumer clientId=consumer-flink4-24, groupId=flink4] Subscribed to partition(s): flink2-0]
[INFO][2021-03-07 01:28:50 382][org.apache.kafka.clients.consumer.internals.SubscriptionState]-[[Consumer clientId=consumer-flink4-24, groupId=flink4] Seeking to EARLIEST offset of partition flink2-0]
[INFO][2021-03-07 01:28:50 382][org.apache.kafka.clients.consumer.internals.SubscriptionState]-[[Consumer clientId=consumer-flink4-23, groupId=flink4] Seeking to EARLIEST offset of partition flink2-1]
[INFO][2021-03-07 01:28:50 393][org.apache.kafka.clients.Metadata]-[[Consumer clientId=consumer-flink4-23, groupId=flink4] Cluster ID: p5zizhkESiOh_qQkrl3oJg]
[INFO][2021-03-07 01:28:50 393][org.apache.kafka.clients.Metadata]-[[Consumer clientId=consumer-flink4-24, groupId=flink4] Cluster ID: p5zizhkESiOh_qQkrl3oJg]
[INFO][2021-03-07 01:28:50 403][org.apache.kafka.clients.consumer.internals.SubscriptionState]-[[Consumer clientId=consumer-flink4-23, groupId=flink4] Resetting offset for partition flink2-1 to offset 0.]
[INFO][2021-03-07 01:28:50 403][org.apache.kafka.clients.consumer.internals.SubscriptionState]-[[Consumer clientId=consumer-flink4-24, groupId=flink4] Resetting offset for partition flink2-0 to offset 0.]
[INFO][2021-03-07 01:28:50 522][io.debezium.connector.mysql.MySqlConnectorTask]-[Found no existing offset, so preparing to perform a snapshot]
[INFO][2021-03-07 01:28:50 562][io.debezium.util.Threads]-[Requested thread factory for connector MySqlConnector, id = mysql_binlog_source named = binlog-client]
[INFO][2021-03-07 01:28:50 571][io.debezium.util.Threads]-[Requested thread factory for connector MySqlConnector, id = mysql_binlog_source named = snapshot]
[INFO][2021-03-07 01:28:50 572][io.debezium.util.Threads]-[Creating thread debezium-mysqlconnector-mysql_binlog_source-snapshot]
[INFO][2021-03-07 01:28:50 573][io.debezium.connector.mysql.SnapshotReader]-[Starting snapshot for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useSSL=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' with locking mode 'minimal']
[INFO][2021-03-07 01:28:50 575][io.debezium.connector.mysql.SnapshotReader]-[Snapshot is using user 'root' with these MySQL grants:]
[INFO][2021-03-07 01:28:50 576][io.debezium.connector.mysql.SnapshotReader]-[	GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, RELOAD, SHUTDOWN, PROCESS, FILE, REFERENCES, INDEX, ALTER, SHOW DATABASES, SUPER, CREATE TEMPORARY TABLES, LOCK TABLES, EXECUTE, REPLICATION SLAVE, REPLICATION CLIENT, CREATE VIEW, SHOW VIEW, CREATE ROUTINE, ALTER ROUTINE, CREATE USER, EVENT, TRIGGER, CREATE TABLESPACE, CREATE ROLE, DROP ROLE ON *.* TO `root`@`%` WITH GRANT OPTION]
[INFO][2021-03-07 01:28:50 576][io.debezium.connector.mysql.SnapshotReader]-[	GRANT APPLICATION_PASSWORD_ADMIN,AUDIT_ADMIN,BACKUP_ADMIN,BINLOG_ADMIN,BINLOG_ENCRYPTION_ADMIN,CLONE_ADMIN,CONNECTION_ADMIN,ENCRYPTION_KEY_ADMIN,GROUP_REPLICATION_ADMIN,INNODB_REDO_LOG_ARCHIVE,PERSIST_RO_VARIABLES_ADMIN,REPLICATION_APPLIER,REPLICATION_SLAVE_ADMIN,RESOURCE_GROUP_ADMIN,RESOURCE_GROUP_USER,ROLE_ADMIN,SERVICE_CONNECTION_ADMIN,SESSION_VARIABLES_ADMIN,SET_USER_ID,SYSTEM_USER,SYSTEM_VARIABLES_ADMIN,TABLE_ENCRYPTION_ADMIN,XA_RECOVER_ADMIN ON *.* TO `root`@`%` WITH GRANT OPTION]
[INFO][2021-03-07 01:28:50 576][io.debezium.connector.mysql.SnapshotReader]-[MySQL server variables related to change data capture:]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	binlog_cache_size                             = 32768                                        ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	binlog_checksum                               = CRC32                                        ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	binlog_direct_non_transactional_updates       = OFF                                          ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	binlog_encryption                             = OFF                                          ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	binlog_error_action                           = ABORT_SERVER                                 ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	binlog_expire_logs_seconds                    = 2592000                                      ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	binlog_format                                 = ROW                                          ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	binlog_group_commit_sync_delay                = 0                                            ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	binlog_group_commit_sync_no_delay_count       = 0                                            ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	binlog_gtid_simple_recovery                   = ON                                           ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	binlog_max_flush_queue_time                   = 0                                            ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	binlog_order_commits                          = ON                                           ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	binlog_rotate_encryption_master_key_at_startup = OFF                                          ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	binlog_row_event_max_size                     = 8192                                         ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	binlog_row_image                              = FULL                                         ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	binlog_row_metadata                           = MINIMAL                                      ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	binlog_row_value_options                      =                                              ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	binlog_rows_query_log_events                  = OFF                                          ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	binlog_stmt_cache_size                        = 32768                                        ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	binlog_transaction_dependency_history_size    = 25000                                        ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	binlog_transaction_dependency_tracking        = COMMIT_ORDER                                 ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	character_set_client                          = utf8mb4                                      ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	character_set_connection                      = utf8mb4                                      ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	character_set_database                        = utf8mb4                                      ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	character_set_filesystem                      = binary                                       ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	character_set_results                         = utf8mb4                                      ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	character_set_server                          = utf8mb4                                      ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	character_set_system                          = utf8                                         ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	character_sets_dir                            = /usr/local/mysql-8.0.19-macos10.15-x86_64/share/charsets/]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	collation_connection                          = utf8mb4_0900_ai_ci                           ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	collation_database                            = utf8mb4_0900_ai_ci                           ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	collation_server                              = utf8mb4_0900_ai_ci                           ]
[INFO][2021-03-07 01:28:50 579][io.debezium.connector.mysql.SnapshotReader]-[	default_collation_for_utf8mb4                 = utf8mb4_0900_ai_ci                           ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	enforce_gtid_consistency                      = OFF                                          ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	gtid_executed                                 =                                              ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	gtid_executed_compression_period              = 1000                                         ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	gtid_mode                                     = OFF                                          ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	gtid_next                                     = AUTOMATIC                                    ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	gtid_owned                                    =                                              ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	gtid_purged                                   =                                              ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	immediate_server_version                      = 999999                                       ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	innodb_api_enable_binlog                      = OFF                                          ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	innodb_version                                = 8.0.19                                       ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	log_statements_unsafe_for_binlog              = ON                                           ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	max_binlog_cache_size                         = 18446744073709547520                         ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	max_binlog_size                               = 1073741824                                   ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	max_binlog_stmt_cache_size                    = 18446744073709547520                         ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	original_server_version                       = 999999                                       ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	protocol_version                              = 10                                           ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	session_track_gtids                           = OFF                                          ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	slave_type_conversions                        =                                              ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	sync_binlog                                   = 1                                            ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	system_time_zone                              = CST                                          ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	time_zone                                     = SYSTEM                                       ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	tls_version                                   = TLSv1,TLSv1.1,TLSv1.2,TLSv1.3                ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	version                                       = 8.0.19                                       ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	version_comment                               = MySQL Community Server - GPL                 ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	version_compile_machine                       = x86_64                                       ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	version_compile_os                            = macos10.15                                   ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[	version_compile_zlib                          = 1.2.11                                       ]
[INFO][2021-03-07 01:28:50 580][io.debezium.connector.mysql.SnapshotReader]-[Step 0: disabling autocommit, enabling repeatable read transactions, and setting lock wait timeout to 10]
[INFO][2021-03-07 01:28:50 585][io.debezium.connector.mysql.SnapshotReader]-[Step 1: flush and obtain global read lock to prevent writes to database]
[INFO][2021-03-07 01:28:50 618][io.debezium.connector.mysql.SnapshotReader]-[Step 2: start transaction with consistent snapshot]
[INFO][2021-03-07 01:28:50 619][io.debezium.connector.mysql.SnapshotReader]-[Step 3: read binlog position of MySQL master]
[INFO][2021-03-07 01:28:50 621][io.debezium.connector.mysql.SnapshotReader]-[	 using binlog 'binlog.000069' at position '228030012' and gtid '']
[INFO][2021-03-07 01:28:50 621][io.debezium.connector.mysql.SnapshotReader]-[Step 4: read list of available databases]
[INFO][2021-03-07 01:28:50 633][io.debezium.connector.mysql.SnapshotReader]-[	 list of available databases is: [datah, ego, hive, information_schema, ke, mysql, performance_schema, sys, traffic]]
[INFO][2021-03-07 01:28:50 633][io.debezium.connector.mysql.SnapshotReader]-[Step 5: read list of available tables in each database]
[INFO][2021-03-07 01:28:50 646][io.debezium.connector.mysql.SnapshotReader]-[	 including 'datah.Rep_qlt_rule_rlst' among known tables]
[INFO][2021-03-07 01:28:50 646][io.debezium.connector.mysql.SnapshotReader]-[	 'datah.Rep_qlt_rule_rlst' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 646][io.debezium.connector.mysql.SnapshotReader]-[	 including 'datah.Rep_qlty_cls' among known tables]
[INFO][2021-03-07 01:28:50 646][io.debezium.connector.mysql.SnapshotReader]-[	 'datah.Rep_qlty_cls' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 646][io.debezium.connector.mysql.SnapshotReader]-[	 including 'datah.Rep_qlty_config' among known tables]
[INFO][2021-03-07 01:28:50 646][io.debezium.connector.mysql.SnapshotReader]-[	 'datah.Rep_qlty_config' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 646][io.debezium.connector.mysql.SnapshotReader]-[	 including 'datah.Rep_qlty_rule' among known tables]
[INFO][2021-03-07 01:28:50 646][io.debezium.connector.mysql.SnapshotReader]-[	 'datah.Rep_qlty_rule' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 647][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ego.manager' among known tables]
[INFO][2021-03-07 01:28:50 647][io.debezium.connector.mysql.SnapshotReader]-[	 'ego.manager' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 648][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ego.tb_content' among known tables]
[INFO][2021-03-07 01:28:50 648][io.debezium.connector.mysql.SnapshotReader]-[	 'ego.tb_content' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 648][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ego.tb_content_category' among known tables]
[INFO][2021-03-07 01:28:50 648][io.debezium.connector.mysql.SnapshotReader]-[	 'ego.tb_content_category' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 648][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ego.tb_item' among known tables]
[INFO][2021-03-07 01:28:50 648][io.debezium.connector.mysql.SnapshotReader]-[	 'ego.tb_item' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 648][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ego.tb_item_cat' among known tables]
[INFO][2021-03-07 01:28:50 648][io.debezium.connector.mysql.SnapshotReader]-[	 'ego.tb_item_cat' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 648][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ego.tb_item_desc' among known tables]
[INFO][2021-03-07 01:28:50 648][io.debezium.connector.mysql.SnapshotReader]-[	 'ego.tb_item_desc' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 648][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ego.tb_item_param' among known tables]
[INFO][2021-03-07 01:28:50 648][io.debezium.connector.mysql.SnapshotReader]-[	 'ego.tb_item_param' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 648][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ego.tb_item_param_item' among known tables]
[INFO][2021-03-07 01:28:50 648][io.debezium.connector.mysql.SnapshotReader]-[	 'ego.tb_item_param_item' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 648][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ego.tb_order' among known tables]
[INFO][2021-03-07 01:28:50 648][io.debezium.connector.mysql.SnapshotReader]-[	 'ego.tb_order' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 648][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ego.tb_order_item' among known tables]
[INFO][2021-03-07 01:28:50 648][io.debezium.connector.mysql.SnapshotReader]-[	 'ego.tb_order_item' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 648][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ego.tb_order_shipping' among known tables]
[INFO][2021-03-07 01:28:50 648][io.debezium.connector.mysql.SnapshotReader]-[	 'ego.tb_order_shipping' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 648][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ego.tb_user' among known tables]
[INFO][2021-03-07 01:28:50 648][io.debezium.connector.mysql.SnapshotReader]-[	 'ego.tb_user' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.AUX_TABLE' among known tables]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.AUX_TABLE' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.BUCKETING_COLS' among known tables]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.BUCKETING_COLS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.CDS' among known tables]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.CDS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.COLUMNS_V2' among known tables]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.COLUMNS_V2' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.COMPACTION_QUEUE' among known tables]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.COMPACTION_QUEUE' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.COMPLETED_COMPACTIONS' among known tables]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.COMPLETED_COMPACTIONS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.COMPLETED_TXN_COMPONENTS' among known tables]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.COMPLETED_TXN_COMPONENTS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.CTLGS' among known tables]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.CTLGS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.DATABASE_PARAMS' among known tables]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.DATABASE_PARAMS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.DB_PRIVS' among known tables]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.DB_PRIVS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.DBS' among known tables]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.DBS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.DELEGATION_TOKENS' among known tables]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.DELEGATION_TOKENS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.FUNC_RU' among known tables]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.FUNC_RU' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.FUNCS' among known tables]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.FUNCS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.GLOBAL_PRIVS' among known tables]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.GLOBAL_PRIVS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.HIVE_LOCKS' among known tables]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.HIVE_LOCKS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.I_SCHEMA' among known tables]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.I_SCHEMA' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.IDXS' among known tables]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.IDXS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.INDEX_PARAMS' among known tables]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.INDEX_PARAMS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.KEY_CONSTRAINTS' among known tables]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.KEY_CONSTRAINTS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 651][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.MASTER_KEYS' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.MASTER_KEYS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.MATERIALIZATION_REBUILD_LOCKS' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.MATERIALIZATION_REBUILD_LOCKS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.METASTORE_DB_PROPERTIES' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.METASTORE_DB_PROPERTIES' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.MIN_HISTORY_LEVEL' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.MIN_HISTORY_LEVEL' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.MV_CREATION_METADATA' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.MV_CREATION_METADATA' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.MV_TABLES_USED' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.MV_TABLES_USED' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.NEXT_COMPACTION_QUEUE_ID' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.NEXT_COMPACTION_QUEUE_ID' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.NEXT_LOCK_ID' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.NEXT_LOCK_ID' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.NEXT_TXN_ID' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.NEXT_TXN_ID' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.NEXT_WRITE_ID' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.NEXT_WRITE_ID' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.NOTIFICATION_LOG' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.NOTIFICATION_LOG' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.NOTIFICATION_SEQUENCE' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.NOTIFICATION_SEQUENCE' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.NUCLEUS_TABLES' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.NUCLEUS_TABLES' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.PART_COL_PRIVS' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.PART_COL_PRIVS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.PART_COL_STATS' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.PART_COL_STATS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.PART_PRIVS' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.PART_PRIVS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.PARTITION_EVENTS' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.PARTITION_EVENTS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.PARTITION_KEY_VALS' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.PARTITION_KEY_VALS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.PARTITION_KEYS' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.PARTITION_KEYS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.PARTITION_PARAMS' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.PARTITION_PARAMS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.PARTITIONS' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.PARTITIONS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.REPL_TXN_MAP' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.REPL_TXN_MAP' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.ROLE_MAP' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.ROLE_MAP' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.ROLES' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.ROLES' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.RUNTIME_STATS' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.RUNTIME_STATS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.SCHEMA_VERSION' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.SCHEMA_VERSION' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.SD_PARAMS' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.SD_PARAMS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.SDS' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.SDS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.SEQUENCE_TABLE' among known tables]
[INFO][2021-03-07 01:28:50 652][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.SEQUENCE_TABLE' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.SERDE_PARAMS' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.SERDE_PARAMS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.SERDES' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.SERDES' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.SKEWED_COL_NAMES' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.SKEWED_COL_NAMES' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.SKEWED_COL_VALUE_LOC_MAP' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.SKEWED_COL_VALUE_LOC_MAP' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.SKEWED_STRING_LIST' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.SKEWED_STRING_LIST' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.SKEWED_STRING_LIST_VALUES' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.SKEWED_STRING_LIST_VALUES' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.SKEWED_VALUES' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.SKEWED_VALUES' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.SORT_COLS' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.SORT_COLS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.TAB_COL_STATS' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.TAB_COL_STATS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.TABLE_PARAMS' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.TABLE_PARAMS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.TBL_COL_PRIVS' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.TBL_COL_PRIVS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.TBL_PRIVS' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.TBL_PRIVS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.TBLS' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.TBLS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.TXN_COMPONENTS' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.TXN_COMPONENTS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.TXN_TO_WRITE_ID' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.TXN_TO_WRITE_ID' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.TXNS' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.TXNS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.TYPE_FIELDS' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.TYPE_FIELDS' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.TYPES' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.TYPES' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.VERSION' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.VERSION' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.WM_MAPPING' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.WM_MAPPING' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.WM_POOL' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.WM_POOL' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.WM_POOL_TO_TRIGGER' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.WM_POOL_TO_TRIGGER' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.WM_RESOURCEPLAN' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.WM_RESOURCEPLAN' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.WM_TRIGGER' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.WM_TRIGGER' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 including 'hive.WRITE_SET' among known tables]
[INFO][2021-03-07 01:28:50 653][io.debezium.connector.mysql.SnapshotReader]-[	 'hive.WRITE_SET' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ke.ke_alarm_clusters' among known tables]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 'ke.ke_alarm_clusters' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ke.ke_alarm_config' among known tables]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 'ke.ke_alarm_config' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ke.ke_alarm_consumer' among known tables]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 'ke.ke_alarm_consumer' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ke.ke_alarm_crontab' among known tables]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 'ke.ke_alarm_crontab' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ke.ke_connect_config' among known tables]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 'ke.ke_connect_config' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ke.ke_consumer_bscreen' among known tables]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 'ke.ke_consumer_bscreen' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ke.ke_consumer_group' among known tables]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 'ke.ke_consumer_group' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ke.ke_consumer_group_summary' among known tables]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 'ke.ke_consumer_group_summary' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ke.ke_logsize' among known tables]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 'ke.ke_logsize' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ke.ke_metrics' among known tables]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 'ke.ke_metrics' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ke.ke_metrics_offline' among known tables]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 'ke.ke_metrics_offline' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ke.ke_p_role' among known tables]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 'ke.ke_p_role' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ke.ke_resources' among known tables]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 'ke.ke_resources' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ke.ke_role_resource' among known tables]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 'ke.ke_role_resource' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 658][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ke.ke_sql_history' among known tables]
[INFO][2021-03-07 01:28:50 659][io.debezium.connector.mysql.SnapshotReader]-[	 'ke.ke_sql_history' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 659][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ke.ke_topic_rank' among known tables]
[INFO][2021-03-07 01:28:50 659][io.debezium.connector.mysql.SnapshotReader]-[	 'ke.ke_topic_rank' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 659][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ke.ke_user_role' among known tables]
[INFO][2021-03-07 01:28:50 659][io.debezium.connector.mysql.SnapshotReader]-[	 'ke.ke_user_role' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 659][io.debezium.connector.mysql.SnapshotReader]-[	 including 'ke.ke_users' among known tables]
[INFO][2021-03-07 01:28:50 659][io.debezium.connector.mysql.SnapshotReader]-[	 'ke.ke_users' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 660][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.columns_priv' among known tables]
[INFO][2021-03-07 01:28:50 660][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.columns_priv' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 660][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.component' among known tables]
[INFO][2021-03-07 01:28:50 660][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.component' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 660][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.db' among known tables]
[INFO][2021-03-07 01:28:50 660][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.db' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 660][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.default_roles' among known tables]
[INFO][2021-03-07 01:28:50 660][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.default_roles' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 660][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.engine_cost' among known tables]
[INFO][2021-03-07 01:28:50 660][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.engine_cost' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 660][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.func' among known tables]
[INFO][2021-03-07 01:28:50 660][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.func' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 660][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.general_log' among known tables]
[INFO][2021-03-07 01:28:50 660][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.general_log' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 660][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.global_grants' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.global_grants' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.gtid_executed' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.gtid_executed' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.help_category' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.help_category' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.help_keyword' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.help_keyword' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.help_relation' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.help_relation' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.help_topic' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.help_topic' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.innodb_index_stats' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.innodb_index_stats' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.innodb_table_stats' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.innodb_table_stats' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.password_history' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.password_history' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.plugin' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.plugin' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.procs_priv' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.procs_priv' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.proxies_priv' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.proxies_priv' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.role_edges' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.role_edges' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.server_cost' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.server_cost' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.servers' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.servers' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.slave_master_info' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.slave_master_info' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.slave_relay_log_info' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.slave_relay_log_info' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.slave_worker_info' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.slave_worker_info' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.slow_log' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.slow_log' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.tables_priv' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.tables_priv' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.time_zone' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.time_zone' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.time_zone_leap_second' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.time_zone_leap_second' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.time_zone_name' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.time_zone_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.time_zone_transition' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.time_zone_transition' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.time_zone_transition_type' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.time_zone_transition_type' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 including 'mysql.user' among known tables]
[INFO][2021-03-07 01:28:50 661][io.debezium.connector.mysql.SnapshotReader]-[	 'mysql.user' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 663][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.accounts' among known tables]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.accounts' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.cond_instances' among known tables]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.cond_instances' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.data_lock_waits' among known tables]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.data_lock_waits' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.data_locks' among known tables]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.data_locks' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_errors_summary_by_account_by_error' among known tables]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_errors_summary_by_account_by_error' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_errors_summary_by_host_by_error' among known tables]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_errors_summary_by_host_by_error' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_errors_summary_by_thread_by_error' among known tables]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_errors_summary_by_thread_by_error' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_errors_summary_by_user_by_error' among known tables]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_errors_summary_by_user_by_error' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_errors_summary_global_by_error' among known tables]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_errors_summary_global_by_error' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_stages_current' among known tables]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_stages_current' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_stages_history' among known tables]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_stages_history' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_stages_history_long' among known tables]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_stages_history_long' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_stages_summary_by_account_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_stages_summary_by_account_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_stages_summary_by_host_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_stages_summary_by_host_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_stages_summary_by_thread_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_stages_summary_by_thread_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_stages_summary_by_user_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_stages_summary_by_user_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_stages_summary_global_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_stages_summary_global_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_statements_current' among known tables]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_statements_current' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_statements_histogram_by_digest' among known tables]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_statements_histogram_by_digest' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_statements_histogram_global' among known tables]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_statements_histogram_global' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_statements_history' among known tables]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_statements_history' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_statements_history_long' among known tables]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_statements_history_long' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 664][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_statements_summary_by_account_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_statements_summary_by_account_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_statements_summary_by_digest' among known tables]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_statements_summary_by_digest' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_statements_summary_by_host_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_statements_summary_by_host_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_statements_summary_by_program' among known tables]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_statements_summary_by_program' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_statements_summary_by_thread_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_statements_summary_by_thread_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_statements_summary_by_user_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_statements_summary_by_user_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_statements_summary_global_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_statements_summary_global_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_transactions_current' among known tables]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_transactions_current' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_transactions_history' among known tables]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_transactions_history' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_transactions_history_long' among known tables]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_transactions_history_long' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_transactions_summary_by_account_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_transactions_summary_by_account_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_transactions_summary_by_host_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_transactions_summary_by_host_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_transactions_summary_by_thread_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_transactions_summary_by_thread_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_transactions_summary_by_user_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_transactions_summary_by_user_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_transactions_summary_global_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_transactions_summary_global_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_waits_current' among known tables]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_waits_current' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_waits_history' among known tables]
[INFO][2021-03-07 01:28:50 665][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_waits_history' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_waits_history_long' among known tables]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_waits_history_long' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_waits_summary_by_account_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_waits_summary_by_account_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_waits_summary_by_host_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_waits_summary_by_host_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_waits_summary_by_instance' among known tables]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_waits_summary_by_instance' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_waits_summary_by_thread_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_waits_summary_by_thread_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_waits_summary_by_user_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_waits_summary_by_user_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.events_waits_summary_global_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.events_waits_summary_global_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.file_instances' among known tables]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.file_instances' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.file_summary_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.file_summary_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.file_summary_by_instance' among known tables]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.file_summary_by_instance' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.global_status' among known tables]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.global_status' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.global_variables' among known tables]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.global_variables' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.host_cache' among known tables]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.host_cache' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.hosts' among known tables]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.hosts' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.keyring_keys' among known tables]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.keyring_keys' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.log_status' among known tables]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.log_status' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.memory_summary_by_account_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.memory_summary_by_account_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.memory_summary_by_host_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.memory_summary_by_host_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.memory_summary_by_thread_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.memory_summary_by_thread_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.memory_summary_by_user_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.memory_summary_by_user_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.memory_summary_global_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.memory_summary_global_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.metadata_locks' among known tables]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.metadata_locks' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.mutex_instances' among known tables]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.mutex_instances' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.objects_summary_global_by_type' among known tables]
[INFO][2021-03-07 01:28:50 666][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.objects_summary_global_by_type' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.performance_timers' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.performance_timers' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.persisted_variables' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.persisted_variables' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.prepared_statements_instances' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.prepared_statements_instances' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.replication_applier_configuration' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.replication_applier_configuration' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.replication_applier_filters' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.replication_applier_filters' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.replication_applier_global_filters' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.replication_applier_global_filters' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.replication_applier_status' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.replication_applier_status' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.replication_applier_status_by_coordinator' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.replication_applier_status_by_coordinator' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.replication_applier_status_by_worker' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.replication_applier_status_by_worker' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.replication_connection_configuration' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.replication_connection_configuration' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.replication_connection_status' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.replication_connection_status' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.replication_group_member_stats' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.replication_group_member_stats' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.replication_group_members' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.replication_group_members' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.rwlock_instances' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.rwlock_instances' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.session_account_connect_attrs' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.session_account_connect_attrs' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.session_connect_attrs' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.session_connect_attrs' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.session_status' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.session_status' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.session_variables' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.session_variables' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.setup_actors' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.setup_actors' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.setup_consumers' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.setup_consumers' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.setup_instruments' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.setup_instruments' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.setup_objects' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.setup_objects' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.setup_threads' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.setup_threads' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.socket_instances' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.socket_instances' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.socket_summary_by_event_name' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.socket_summary_by_event_name' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.socket_summary_by_instance' among known tables]
[INFO][2021-03-07 01:28:50 667][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.socket_summary_by_instance' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.status_by_account' among known tables]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.status_by_account' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.status_by_host' among known tables]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.status_by_host' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.status_by_thread' among known tables]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.status_by_thread' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.status_by_user' among known tables]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.status_by_user' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.table_handles' among known tables]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.table_handles' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.table_io_waits_summary_by_index_usage' among known tables]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.table_io_waits_summary_by_index_usage' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.table_io_waits_summary_by_table' among known tables]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.table_io_waits_summary_by_table' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.table_lock_waits_summary_by_table' among known tables]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.table_lock_waits_summary_by_table' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.threads' among known tables]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.threads' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.user_defined_functions' among known tables]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.user_defined_functions' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.user_variables_by_thread' among known tables]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.user_variables_by_thread' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.users' among known tables]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.users' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.variables_by_thread' among known tables]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.variables_by_thread' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 including 'performance_schema.variables_info' among known tables]
[INFO][2021-03-07 01:28:50 668][io.debezium.connector.mysql.SnapshotReader]-[	 'performance_schema.variables_info' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 671][io.debezium.connector.mysql.SnapshotReader]-[	 including 'sys.sys_config' among known tables]
[INFO][2021-03-07 01:28:50 671][io.debezium.connector.mysql.SnapshotReader]-[	 'sys.sys_config' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 673][io.debezium.connector.mysql.SnapshotReader]-[	 including 'traffic.soretable' among known tables]
[INFO][2021-03-07 01:28:50 673][io.debezium.connector.mysql.SnapshotReader]-[	 'traffic.soretable' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 673][io.debezium.connector.mysql.SnapshotReader]-[	 including 'traffic.t_average_speed' among known tables]
[INFO][2021-03-07 01:28:50 673][io.debezium.connector.mysql.SnapshotReader]-[	 'traffic.t_average_speed' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 673][io.debezium.connector.mysql.SnapshotReader]-[	 including 'traffic.t_monitor_info' among known tables]
[INFO][2021-03-07 01:28:50 673][io.debezium.connector.mysql.SnapshotReader]-[	 'traffic.t_monitor_info' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 673][io.debezium.connector.mysql.SnapshotReader]-[	 including 'traffic.t_speeding_info' among known tables]
[INFO][2021-03-07 01:28:50 673][io.debezium.connector.mysql.SnapshotReader]-[	 'traffic.t_speeding_info' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 673][io.debezium.connector.mysql.SnapshotReader]-[	 including 'traffic.t_violation_list' among known tables]
[INFO][2021-03-07 01:28:50 673][io.debezium.connector.mysql.SnapshotReader]-[	 'traffic.t_violation_list' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 673][io.debezium.connector.mysql.SnapshotReader]-[	 including 'traffic.test_flink_sink' among known tables]
[INFO][2021-03-07 01:28:50 673][io.debezium.connector.mysql.SnapshotReader]-[	 'traffic.test_flink_sink' is filtered out of capturing]
[INFO][2021-03-07 01:28:50 673][io.debezium.connector.mysql.SnapshotReader]-[	 including 'traffic.testflink' among known tables]
[INFO][2021-03-07 01:28:50 673][io.debezium.connector.mysql.SnapshotReader]-[	 including 'traffic.testflink' for further processing]
[INFO][2021-03-07 01:28:50 675][io.debezium.connector.mysql.SnapshotReader]-[	snapshot continuing with database(s): [traffic]]
[INFO][2021-03-07 01:28:50 675][io.debezium.connector.mysql.SnapshotReader]-[Step 6: generating DROP and CREATE statements to reflect current database schemas:]
[INFO][2021-03-07 01:28:52 680][io.debezium.connector.mysql.SnapshotReader]-[Step 7: releasing global read lock to enable MySQL writes]
[INFO][2021-03-07 01:28:52 681][io.debezium.connector.mysql.SnapshotReader]-[Step 7: blocked writes to MySQL for a total of 00:00:02.063]
[INFO][2021-03-07 01:28:52 682][io.debezium.connector.mysql.SnapshotReader]-[Step 8: scanning contents of 1 tables while still in transaction]
[INFO][2021-03-07 01:28:52 695][io.debezium.connector.mysql.SnapshotReader]-[Step 8: - scanning table 'traffic.testflink' (1 of 1 tables)]
[INFO][2021-03-07 01:28:52 695][io.debezium.connector.mysql.SnapshotReader]-[For table 'traffic.testflink' using select statement: 'SELECT * FROM `traffic`.`testflink`']
[INFO][2021-03-07 01:28:52 705][io.debezium.connector.mysql.SnapshotReader]-[Step 8: - Completed scanning a total of 9 rows from table 'traffic.testflink' after 00:00:00.01]
[INFO][2021-03-07 01:28:52 705][io.debezium.connector.mysql.SnapshotReader]-[Step 8: scanned 9 rows in 1 tables in 00:00:00.023]
[INFO][2021-03-07 01:28:52 705][io.debezium.connector.mysql.SnapshotReader]-[Step 9: committing transaction]
[INFO][2021-03-07 01:28:52 707][io.debezium.connector.mysql.SnapshotReader]-[Completed snapshot in 00:00:02.134]
[INFO][2021-03-07 01:28:53 042][com.alibaba.ververica.cdc.debezium.internal.DebeziumChangeConsumer]-[Database snapshot phase can't perform checkpoint, acquired Checkpoint lock.]
[INFO][2021-03-07 01:28:53 047][com.alibaba.ververica.cdc.debezium.internal.DebeziumChangeConsumer]-[Received record from streaming binlog phase, released checkpoint lock.]
[INFO][2021-03-07 01:28:53 048][io.debezium.connector.mysql.ChainedReader]-[Transitioning from the snapshot reader to the binlog reader]
[INFO][2021-03-07 01:28:53 059][io.debezium.util.Threads]-[Creating thread debezium-mysqlconnector-mysql_binlog_source-binlog-client]
[INFO][2021-03-07 01:28:53 063][io.debezium.util.Threads]-[Creating thread debezium-mysqlconnector-mysql_binlog_source-binlog-client]
[INFO][2021-03-07 01:28:53 078][io.debezium.connector.mysql.BinlogReader]-[Connected to MySQL binlog at localhost:3306, starting at binlog file 'binlog.000069', pos=228030012, skipping 0 events plus 0 rows]
[INFO][2021-03-07 01:28:53 078][io.debezium.connector.mysql.BinlogReader]-[Waiting for keepalive thread to start]
[INFO][2021-03-07 01:28:53 079][io.debezium.util.Threads]-[Creating thread debezium-mysqlconnector-mysql_binlog_source-binlog-client]
[INFO][2021-03-07 01:28:53 183][io.debezium.connector.mysql.BinlogReader]-[Keepalive thread is running]
[INFO][2021-03-07 01:28:55 464][org.apache.kafka.clients.consumer.internals.AbstractCoordinator]-[[Consumer clientId=consumer-flink4-23, groupId=flink4] Discovered group coordinator node04:9092 (id: 2147483645 rack: null)]
[INFO][2021-03-07 01:28:55 464][org.apache.kafka.clients.consumer.internals.AbstractCoordinator]-[[Consumer clientId=consumer-flink4-24, groupId=flink4] Discovered group coordinator node04:9092 (id: 2147483645 rack: null)]
[INFO][2021-03-07 01:29:41 052][io.debezium.connector.mysql.BinlogReader]-[1 records sent during previous 00:00:47.993, last recorded offset: {ts_sec=1615051780, file=binlog.000069, pos=228034832, row=1, server_id=1, event=2}]
[INFO][2021-03-07 01:30:16 550][io.debezium.connector.mysql.BinlogReader]-[1 records sent during previous 00:00:35.499, last recorded offset: {ts_sec=1615051816, file=binlog.000069, pos=228038954, row=1, server_id=1, event=2}]
[INFO][2021-03-07 01:31:26 135][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]-[Shutting down TaskExecutorLocalStateStoresManager.]
[INFO][2021-03-07 01:31:26 136][org.apache.flink.runtime.blob.TransientBlobCache]-[Shutting down BLOB cache]
[INFO][2021-03-07 01:31:26 144][org.apache.flink.runtime.blob.PermanentBlobCache]-[Shutting down BLOB cache]
[INFO][2021-03-07 01:31:26 199][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]-[FileChannelManager removed spill file directory /var/folders/r3/pbs1kc5s4b53dx4mw86904gw0000gn/T/flink-netty-shuffle-b7eaa46c-63c0-423f-8b58-9b5fb9b82f45]
[INFO][2021-03-07 01:31:26 202][org.apache.flink.runtime.filecache.FileCache]-[removed file cache directory /var/folders/r3/pbs1kc5s4b53dx4mw86904gw0000gn/T/flink-dist-cache-84e919d8-414c-4bd5-a51a-aa85ef81dd28]
[INFO][2021-03-07 01:31:26 204][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]-[FileChannelManager removed spill file directory /var/folders/r3/pbs1kc5s4b53dx4mw86904gw0000gn/T/flink-io-3051d246-88a7-4655-89d0-d5f50db48be0]
[INFO][2021-03-07 01:31:26 207][org.apache.flink.runtime.blob.BlobServer]-[Stopped BLOB server at 0.0.0.0:49633]
